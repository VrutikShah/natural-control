{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping generating vocabulary file for ./data/vocab200.context\n",
      "158 158\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import logging\n",
    "# import torch\n",
    "# from model import QAModel\n",
    "# from train import train\n",
    "import pickle\n",
    "# from train import train\n",
    "# from model import QAModel\n",
    "\n",
    "from preprocess import get_glove, create_vocabulary, create_vocab_class\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "NUM_EPOCH = 3\n",
    "LEARNING_RATE =0.001\n",
    "BATCH_SIZE = 16\n",
    "HIDDEN_SIZE = 128\n",
    "CONTEXT_LEN = 300\n",
    "QUESTION_LEN = 150\n",
    "ANSWER_LEN = 50\n",
    "EMBEDDING_SIZE = 100\n",
    "LOAD_PREV = False\n",
    "# SAMPLING CONSTANTS\n",
    "\n",
    "with open(\"./data/emb_matrix.pkl\", \"rb\") as f:\n",
    "    emb_matrix = pickle.load(f)\n",
    "\n",
    "with open(\"./data/word2id.pkl\", \"rb\") as f:\n",
    "    word2id=  pickle.load(f)\n",
    "\n",
    "with open(\"./data/id2word.pkl\", \"rb\") as f:\n",
    "    id2word = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "context_vocab_path = \"./data/vocab200.context\"\n",
    "train_context_path = \"./data/train.graph\"\n",
    "context_vocab, rev_context_vocab = create_vocabulary(context_vocab_path,train_context_path,200)\n",
    "NO_CLASS = len(context_vocab)\n",
    "\n",
    "# TODO \n",
    "# qa_model.train(train_context_path, train_qn_path, train_ans_path, dev_qn_path, dev_context_path, dev_ans_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Training Steps\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "def loss(loss_func,logits,target):\n",
    "    return loss_func(logits,target)\n",
    "\n",
    "\n",
    "\n",
    "def train_iteration(qamodel,batch,criterion,encoder_optimizer,decoder_optimizer):\n",
    "    context_ids = batch.context_ids\n",
    "    qn_ids = batch.qn_ids\n",
    "    ans_ids = batch.ans_ids\n",
    "    qn_mask = batch.qn_mask\n",
    "    batch_size = batch.batch_size\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "#     qn_ids = torch.from_numpy(qn_ids).long().to(device)\n",
    "    context_ids = torch.from_numpy(context_ids).long().to(device)\n",
    "#     ans_ids = torch.from_numpy(ans_ids).long().to(device)\n",
    "#     qn_mask = torch.from_numpy(qn_mask).long().to(device)\n",
    "    \n",
    "#     print(f\"qn_ids - {qn_ids.is_cuda}, context_ids - {context_ids.is_cuda}, ans_ids - {ans_ids.is_cuda}, qn_mask - {qn_mask.is_cuda}\")\n",
    "    decoder_outputs= qamodel(qn_ids,context_ids,ans_ids,qn_mask)\n",
    "    loss = 0    #loss per batch\n",
    "    # print(len(decoder_outputs))\n",
    "    ans_ids = torch.tensor(ans_ids).transpose(0,1).to(device)\n",
    "    for idx,dec_out in enumerate(decoder_outputs):\n",
    "        # print(dec_out.shape)\n",
    "        # dec_out = [bsz,output_vocab_size]\n",
    "        # print(ans_ids[idx],ans_ids[idx].shape)\n",
    "        loss += criterion(dec_out,ans_ids[idx])\n",
    "    \n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item()/batch_size\n",
    "\n",
    "\n",
    "def train(qamodel,num_epochs, context_path, qn_path, ans_path, batch_size):\n",
    "    epoch = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    encoder_optimizer = optim.Adam(qamodel.encoder.parameters(),lr = 0.001)\n",
    "    decoder_optimizer = optim.Adam(qamodel.decoder.parameters(),lr = 0.001)\n",
    "    # initialise optimiser\n",
    "    while epoch<num_epochs:\n",
    "        epoch+=1\n",
    "        epoch_loss = 0\n",
    "        epoch_start_time = time.time()\n",
    "        num_iters = 0\n",
    "        for batch in get_batch_generator(qamodel.word2id, qamodel.context2id, qamodel.ans2id, context_path,\n",
    "                                            qn_path, ans_path, batch_size, qamodel.graph_vocab_class,\n",
    "                                            context_len=300, question_len=150,\n",
    "                                            answer_len=50, discard_long=False):\n",
    "            try:\n",
    "\n",
    "                batch_loss  = train_iteration(qamodel,batch,criterion,encoder_optimizer,decoder_optimizer)\n",
    "                num_iters += 1\n",
    "                epoch_loss += batch_loss\n",
    "                if num_iters%50==0:\n",
    "                    print(f'End of {num_iters} batches with loss = {batch_loss}')\n",
    "            except RuntimeError as e:\n",
    "                print(e)\n",
    "                continue\n",
    "            # loss backward\n",
    "            # optimiser step\n",
    "            # if num_iters%print_every:\n",
    "\n",
    "            # add line to print at print_every\n",
    "        print('End of epoch',epoch,' | Loss = ',epoch_loss)\n",
    "        epoch_end_time = time.time()\n",
    "        time_of_epoch = epoch_end_time - epoch_start_time\n",
    "        print(time_of_epoch)\n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab.py\n",
    "\n",
    "\"\"\"This file contains a function to read the GloVe vectors from file,\n",
    "and return them as an embedding matrix\"\"\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "_PAD = r\"<pad>\"\n",
    "_UNK = r\"<unk>\"\n",
    "_SOS = r\"<sos>\"\n",
    "_START_VOCAB = [_PAD, _UNK, _SOS]\n",
    "PAD_ID = 0\n",
    "UNK_ID = 1\n",
    "SOS_ID = 2\n",
    "\n",
    "# Regular expressions used to tokenize.\n",
    "_WORD_SPLIT = re.compile(r\"([.,!?\\\"':;()/-])\")\n",
    "_DIGIT_RE = re.compile(r\"\\d\")\n",
    "_ATTRIBUTE_RE = r'\\d+[A-Za-z]+'\n",
    "_NODE_RE = r'^[O|R|K|B|C|H|L]-\\d+'\n",
    "_ACT_RE = r'[A-Za-z]+'\n",
    "_DISCARD_TOK = ['(', ')', 'nt', ';']\n",
    "\n",
    "\n",
    "class Vocab:\n",
    "    \"\"\"\n",
    "    Read in the class Vocab, it is a tidied class containing classified elements in graph\n",
    "    self.node2id: dictionary converting the node token to its id\n",
    "    self.nodes: list of all node tokens\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, node2id, edge2id, flag2id):\n",
    "        self.discard_tokens = _DISCARD_TOK\n",
    "        self.node2id = node2id\n",
    "        self.edge2id = edge2id\n",
    "        self.flag2id = flag2id\n",
    "        self.nodes = list(node2id.keys())\n",
    "        self.edges = list(edge2id.keys())\n",
    "        self.flags = list(flag2id.keys())\n",
    "        self.all_tokens = self.flags + self.nodes + self.edges\n",
    "\n",
    "    def tidy_in_triplet(self, tokens):\n",
    "        \"\"\"\n",
    "        convert raw tokens into a id list of length [3*N]\n",
    "        [first_node_id_list, edge_id_list, second_id_list, first_id_list, edge_id_list, second_id_list, ...]\n",
    "        each entry in the list contains another list because node can be composed of many elements\n",
    "        [[node_element0, node_element1, ...], [edge_element0, edge_element1, ...], [...], ...]\n",
    "        :param tokens: a list of raw tokens in graph txt file\n",
    "        :return: a list of lists\n",
    "        \"\"\"\n",
    "        ids = []\n",
    "        for (i, w) in enumerate(tokens):\n",
    "            if w in self.nodes:\n",
    "                if (i == 0) or (tokens[i - 1] in [';', ')']) or (tokens[i - 1] in self.edges):\n",
    "                    ids.append([self.node2id[w]])\n",
    "                else:\n",
    "                    ids[-1].append(self.node2id[w])\n",
    "            elif w in self.edges:\n",
    "                if (tokens[i - 1] in self.flags) or (tokens[i - 1] in self.nodes):\n",
    "                    ids.append([self.edge2id[w]])\n",
    "                else:\n",
    "                    ids[-1].append(self.edge2id[w])\n",
    "            elif w in ['l', 'r']:\n",
    "                w = \"-\".join(tokens[i - 1: i + 1])\n",
    "                if w in self.edges:\n",
    "                    ids[-1].append(self.edge2id[w])\n",
    "                else:\n",
    "                    ids[-1].append(UNK_ID)\n",
    "                    \n",
    "            elif (w in self.discard_tokens) or (re.match(_ATTRIBUTE_RE, w)):\n",
    "                if w == ';':\n",
    "                    assert len(ids) % 3 == 0, \"error in tidy_in_triplet, can't be divided by 3\"\n",
    "                continue\n",
    "            elif w not in self.all_tokens:\n",
    "                raise ValueError(\"new token %s in graph representation.\"%w)\n",
    "        return ids\n",
    "\n",
    "def get_glove(glove_path, glove_dim):\n",
    "    \"\"\"Reads from original GloVe .txt file and returns embedding matrix and\n",
    "    mappings from words to word ids.\n",
    "\n",
    "    Input:\n",
    "      glove_path: path to glove.6B.{glove_dim}d.txt\n",
    "      glove_dim: integer; needs to match the dimension in glove_path\n",
    "\n",
    "    Returns:\n",
    "      emb_matrix: Numpy array shape (400002, glove_dim) containing glove embeddings\n",
    "(plus PAD and UNK embeddings in first two rows).\n",
    "        The rows of emb_matrix correspond to the word ids given in word2id and id2word\n",
    "      word2id: dictionary mapping word (string) to word id (int)\n",
    "      id2word: dictionary mapping word id (int) to word (string)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Loading GLoVE vectors from file: %s\" % glove_path)\n",
    "    vocab_size = int(4e5)  # this is the vocab size of the corpus we've downloaded\n",
    "\n",
    "    emb_matrix = np.zeros((vocab_size + len(_START_VOCAB), glove_dim))\n",
    "    word2id = {}\n",
    "    id2word = {}\n",
    "\n",
    "    random_init = True\n",
    "    # randomly initialize the special tokens\n",
    "    if random_init:\n",
    "        emb_matrix[:len(_START_VOCAB), :] = np.random.randn(len(_START_VOCAB), glove_dim)\n",
    "\n",
    "    # put start tokens in the dictionaries\n",
    "    idx = 0\n",
    "    for word in _START_VOCAB:\n",
    "        word2id[word] = idx\n",
    "        id2word[idx] = word\n",
    "        idx += 1\n",
    "\n",
    "    # go through glove vecs\n",
    "    with open(glove_path, 'r', encoding=\"utf-8\") as fh:\n",
    "        for line in tqdm(fh, total=vocab_size):\n",
    "            line = line.lstrip().rstrip().split(\" \")\n",
    "            word = line[0]\n",
    "            vector = list(map(float, line[1:]))\n",
    "            if glove_dim != len(vector):\n",
    "                raise Exception(\n",
    "                    \"You set --glove_path=%s but --embedding_size=%i. If you set --glove_path yourself then make sure that --embedding_size matches!\" % (\n",
    "                    glove_path, glove_dim))\n",
    "            emb_matrix[idx, :] = vector\n",
    "            word2id[word] = idx\n",
    "            id2word[idx] = word\n",
    "            idx += 1\n",
    "\n",
    "    final_vocab_size = vocab_size + len(_START_VOCAB)\n",
    "    assert len(word2id) == final_vocab_size\n",
    "    assert len(id2word) == final_vocab_size\n",
    "    assert idx == final_vocab_size\n",
    "\n",
    "    return emb_matrix, word2id, id2word\n",
    "\n",
    "\n",
    "def one_hot_converter(vec_len):\n",
    "    one_hot_embed = np.zeros((vec_len, vec_len))\n",
    "    np.fill_diagonal(one_hot_embed, 1)\n",
    "    return one_hot_embed\n",
    "\n",
    "def instruction_tokenizer(sentence):\n",
    "    \"\"\"\n",
    "    A special tokenizer for instructions.\n",
    "    Turn into lower case and split Office-1 or office1 into \"Office 1\",\n",
    "    :param sentence: instructions (natural language)\n",
    "    :return: a list of tokens\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    prepocessed_sen_list = preprocess_instruction(sentence.strip())\n",
    "    for space_separated_fragment in prepocessed_sen_list:\n",
    "        words.extend(_WORD_SPLIT.split(space_separated_fragment))\n",
    "    return [w.lower() for w in words if w]\n",
    "\n",
    "def preprocess_instruction(sentence):\n",
    "    # change \"office-12\" or \"office12\" to \"office 12\"\n",
    "    # change \"12-office\" or \"12office\" to \"12 office\"\n",
    "    _WORD_NO_SPACE_NUM_RE = r'([A-Za-z]+)\\-?(\\d+)'\n",
    "    _NUM_NO_SPACE_WORD_RE = r'(\\d+)\\-?([A-Za-z]+)'\n",
    "    new_str = re.sub(_WORD_NO_SPACE_NUM_RE, lambda m: m.group(1) + ' ' + m.group(2), sentence)\n",
    "    new_str = re.sub(_NUM_NO_SPACE_WORD_RE, lambda m: m.group(1) + ' ' + m.group(2), new_str)\n",
    "    lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "    # correct common typos.\n",
    "    correct_error_dic = {'rom': 'room', 'gout': 'go out', 'roo': 'room',\n",
    "                         'immeidately': 'immediately', 'halway': 'hallway',\n",
    "                         'office-o': 'office 0', 'hall-o': 'hall 0', 'pas': 'pass',\n",
    "                         'offic': 'office', 'leftt': 'left', 'iffice': 'office'}\n",
    "    for err_w in correct_error_dic:\n",
    "        find_w = ' ' + err_w + ' '\n",
    "        replace_w = ' ' + correct_error_dic[err_w] + ' '\n",
    "        new_str = new_str.replace(find_w, replace_w)\n",
    "    sen_list = []\n",
    "    # Lemmatize words\n",
    "    for word in new_str.split(' '):\n",
    "        try:\n",
    "            word = lemma.lemmatize(word)\n",
    "            if len(word) > 0 and word[-1] == '-':\n",
    "                word = word[:-1]\n",
    "            if word:\n",
    "                sen_list.append(word)\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "            # print(\"unicode error \", word, new_str)\n",
    "    return sen_list\n",
    "\n",
    "def basic_tokenizer(sentence):\n",
    "    \"\"\"Very basic tokenizer: split the sentence into a list of tokens.\"\"\"\n",
    "    words = []\n",
    "    for space_separated_fragment in sentence.strip().split():\n",
    "        words.extend(_WORD_SPLIT.split(space_separated_fragment))\n",
    "    return [w for w in words if w]\n",
    "\n",
    "def create_vocab_class(vocab_dict):\n",
    "    \"\"\"\n",
    "    Convert the raw tokens in graph to\n",
    "    To organize the vocab into two groups: node group, action + attribute group\n",
    "    :param vocab_dict:\n",
    "    :param rev_vocab:\n",
    "    :return: A vocab class holding all the information necessary for training\n",
    "    \"\"\"\n",
    "    rev_vocab = vocab_dict.keys()\n",
    "    new_vocab_dic = {\"node\": [\"S\", \"N\", \"E\", \"W\"], \"edge\": [], \"flag\": _START_VOCAB}\n",
    "    for vocab in rev_vocab:\n",
    "        if vocab in 'lrSNEW':\n",
    "            continue\n",
    "        elif re.match(_NODE_RE, vocab):\n",
    "            new_vocab_dic[\"node\"].append(vocab)\n",
    "        elif re.match(_ATTRIBUTE_RE, vocab):\n",
    "            new_vocab_dic[\"edge\"].append(vocab + '-l')\n",
    "            new_vocab_dic[\"edge\"].append(vocab + '-r')\n",
    "        elif re.match(_ACT_RE, vocab) and vocab != 'nt':\n",
    "            new_vocab_dic[\"edge\"].append(vocab)\n",
    "    node2id = dict([(x, y) for (y, x) in enumerate(new_vocab_dic['node'])])\n",
    "    edge2id = dict([(x, y) for (y, x) in enumerate(new_vocab_dic['edge'])])\n",
    "    flag2id = dict([(x, y) for (y, x) in enumerate(new_vocab_dic['flag'])])\n",
    "\n",
    "    return Vocab(node2id, edge2id, flag2id)\n",
    "\n",
    "\n",
    "def create_vocabulary(vocabulary_path, data_path, max_vocabulary_size,\n",
    "                      tokenizer=None, normalize_digits=False):\n",
    "    \"\"\"Create vocabulary file (if it does not exist yet) from data file.\n",
    "\n",
    "    Data file is assumed to contain one sentence per line. Each sentence is\n",
    "    tokenized and digits are normalized (if normalize_digits is set).\n",
    "    Vocabulary contains the most-frequent tokens up to max_vocabulary_size.\n",
    "    We write it to vocabulary_path in a one-token-per-line format, so that later\n",
    "    token in the first line gets id=0, second line gets id=1, and so on.\n",
    "\n",
    "    Args:\n",
    "      vocabulary_path: path where the vocabulary will be created.\n",
    "      data_path: data file that will be used to create vocabulary.\n",
    "      max_vocabulary_size: limit on the size of the created vocabulary.\n",
    "      tokenizer: a function to use to tokenize each data sentence;\n",
    "        if None, basic_tokenizer will be used.\n",
    "      normalize_digits: Boolean; if true, all digits are replaced by 0s.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(vocabulary_path):\n",
    "        print(\"Creating vocabulary %s from data %s\" % (vocabulary_path, data_path))\n",
    "        vocab = {}\n",
    "        with open(data_path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            counter = 0\n",
    "            for line in f:\n",
    "                counter += 1\n",
    "                if counter % 100000 == 0:\n",
    "                    print(\"  processing line %d\" % counter)\n",
    "                \n",
    "                # TODO - CHANGE THE BELOW LINE\n",
    "                # line = tf.compat.as_bytes(line)\n",
    "                tokens = tokenizer(line) if tokenizer else basic_tokenizer(line)\n",
    "                for w in tokens:\n",
    "                    word = _DIGIT_RE.sub(r\"0\", w) if normalize_digits else w\n",
    "                    if word in vocab:\n",
    "                        vocab[word] += 1\n",
    "                    else:\n",
    "                        vocab[word] = 1\n",
    "            vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n",
    "            if len(vocab_list) > max_vocabulary_size:\n",
    "                vocab_list = vocab_list[:max_vocabulary_size]\n",
    "            with open(vocabulary_path, mode=\"w\", encoding=\"utf-8\") as vocab_file:\n",
    "                for w in vocab_list:\n",
    "                    vocab_file.write(w + b\"\\n\")\n",
    "        rev_vocab = vocab_list # a list contain all the tokens\n",
    "        vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])  # key is the token, value is index\n",
    "        return vocab, rev_vocab\n",
    "    else:\n",
    "        print(\"Skipping generating vocabulary file for {}\".format(vocabulary_path))\n",
    "        return initialize_vocabulary(vocabulary_path)\n",
    "\n",
    "def initialize_vocabulary(vocabulary_path):\n",
    "  \"\"\"Initialize vocabulary from file.\n",
    "\n",
    "  We assume the vocabulary is stored one-item-per-line, so a file:\n",
    "    dog\n",
    "    cat\n",
    "  will result in a vocabulary {\"dog\": 0, \"cat\": 1}, and this function will\n",
    "  also return the reversed-vocabulary [\"dog\", \"cat\"].\n",
    "\n",
    "  Args:\n",
    "    vocabulary_path: path to the file containing the vocabulary.\n",
    "\n",
    "  Returns:\n",
    "    a pair: the vocabulary (a dictionary mapping string to integers), and\n",
    "    the reversed vocabulary (a list, which reverses the vocabulary mapping).\n",
    "\n",
    "  Raises:\n",
    "    ValueError: if the provided vocabulary_path does not exist.\n",
    "  \"\"\"\n",
    "  if os.path.exists(vocabulary_path):\n",
    "    rev_vocab = []\n",
    "    with open(vocabulary_path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "      rev_vocab.extend(f.readlines())\n",
    "    \n",
    "    # TODO - CHANGE THE BELOW LINE\n",
    "    # rev_vocab = [tf.compat.as_bytes(line.strip()) for line in rev_vocab]\n",
    "    rev_vocab = [line.strip() for line in rev_vocab]\n",
    "    vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n",
    "    print(len(vocab.keys()), len(rev_vocab))\n",
    "    return vocab, rev_vocab\n",
    "  else:\n",
    "    raise ValueError(\"Vocabulary file %s not found.\", vocabulary_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This file contains code to read tokenized data from file,\n",
    "truncate, pad and process it into batches ready for training\"\"\"\n",
    "\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Batch(object):\n",
    "    \"\"\"A class to hold the information needed for a training batch\"\"\"\n",
    "\n",
    "    def __init__(self, context_ids, context_tokens, qn_ids, qn_mask, qn_tokens, ans_ids, ans_mask,\n",
    "                 ans_tokens, batch_size):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "          {context/qn}_ids: Numpy arrays.\n",
    "            Shape (batch_size, {context_len/question_len}). Contains padding.\n",
    "          {context/qn}_mask: Numpy arrays, same shape as _ids.\n",
    "            Contains 1s where there is real data, 0s where there is padding.\n",
    "          {context/qn/ans}_tokens: Lists length batch_size, containing lists (unpadded) of tokens (strings)\n",
    "          ans_span: numpy array, shape (batch_size, 2)\n",
    "        \"\"\"\n",
    "        self.context_ids = context_ids\n",
    "        # self.context_mask = context_mask\n",
    "        self.context_tokens = context_tokens\n",
    "        # self.context_embeddings = context_embeddings\n",
    "\n",
    "        self.qn_ids = qn_ids\n",
    "        self.qn_mask = qn_mask\n",
    "        self.qn_tokens = qn_tokens\n",
    "\n",
    "        self.ans_ids = ans_ids\n",
    "        self.ans_mask = ans_mask\n",
    "        self.ans_tokens = ans_tokens\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "\n",
    "def split_by_whitespace(sentence):\n",
    "    words = []\n",
    "    for space_separated_fragment in sentence.strip().split():\n",
    "        words.extend(re.split(\" \", space_separated_fragment))\n",
    "    return [w for w in words if w]\n",
    "\n",
    "\n",
    "def intstr_to_intlist(string):\n",
    "    \"\"\"Given a string e.g. '311 9 1334 635 6192 56 639', returns as a list of integers\"\"\"\n",
    "    return [int(s) for s in string.split()]\n",
    "\n",
    "\n",
    "def sentence_to_token_ids(sentence, word2id, is_instr=False):\n",
    "    \"\"\"Turns an already-tokenized sentence string into word indices\n",
    "    e.g. \"i do n't know\" -> [9, 32, 16, 96]\n",
    "    Note any token that isn't in the word2id mapping gets mapped to the id for UNK\n",
    "    \"\"\"\n",
    "    if is_instr:\n",
    "        tokens = instruction_tokenizer(sentence)  # list of strings\n",
    "    else:\n",
    "        tokens = split_by_whitespace(sentence)\n",
    "\n",
    "    # if simply split in tokens.\n",
    "    ids = [word2id.get(w, UNK_ID) for w in tokens]\n",
    "    ''' for debugging\n",
    "    if UNK_ID in ids:\n",
    "        print(tokens[ids.index(UNK_ID)], \" \".join(tokens))\n",
    "    '''\n",
    "    return tokens, ids\n",
    "\n",
    "def padded(token_batch, batch_pad=0):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "      token_batch: List (length batch size) of lists of ints.\n",
    "      batch_pad: Int. Length to pad to. If 0, pad to maximum length sequence in token_batch.\n",
    "    Returns:\n",
    "      List (length batch_size) of padded of lists of ints.\n",
    "        All are same length - batch_pad if batch_pad!=0, otherwise the maximum length in token_batch\n",
    "    \"\"\"\n",
    "    maxlen = max(map(lambda x: len(x), token_batch)) if batch_pad == 0 else batch_pad\n",
    "    return map(lambda token_list: token_list + [PAD_ID] * (maxlen - len(token_list)), token_batch)\n",
    "\n",
    "def reorganize(context_line, ans_line):\n",
    "    start = ans_line.strip().split()[0]\n",
    "    context_trip_list = context_line.strip().split(';')\n",
    "    trips_contain_start = []\n",
    "    trips_not_contain_start = []\n",
    "\n",
    "    for trip_str in context_trip_list:\n",
    "        if start in trip_str:\n",
    "            trips_contain_start.append(trip_str)\n",
    "        else:\n",
    "            trips_not_contain_start.append(trip_str)\n",
    "    if trips_not_contain_start[0][0] != ' ':\n",
    "        trips_not_contain_start[0] = ' ' + trips_not_contain_start[0]\n",
    "    organized_context_line = \";\".join(trips_contain_start + trips_not_contain_start).strip() + '\\n'\n",
    "\n",
    "    #assert len(organized_context_line) == len(context_line), \"len {} {}{} len {}\".\\\n",
    "    #      format(len(context_line), context_line, organized_context_line, len(organized_context_line))\n",
    "    return organized_context_line\n",
    "\n",
    "\n",
    "def refill_batches(batches, word2id, context2id, ans2id, context_file, qn_file, ans_file, batch_size, context_len,\n",
    "                   question_len, ans_len, discard_long, shuffle=True, output_goal=False):\n",
    "    \"\"\"\n",
    "    Adds more batches into the \"batches\" list.\n",
    "    Inputs:\n",
    "      batches: list to add batches to\n",
    "      word2id: dictionary mapping word (string) to word id (int)\n",
    "      context_file, qn_file, ans_file: paths to {train/dev}.{context/question/answer} data files\n",
    "      batch_size: int. how big to make the batches\n",
    "      context_len, question_len: max length of context and question respectively\n",
    "      discard_long: If True, discard any examples that are longer than context_len or question_len.\n",
    "        If False, truncate those exmaples instead.\n",
    "    \"\"\"\n",
    "    print(\"Refilling batches...\")\n",
    "    tic = time.time()\n",
    "    examples = []  # list of (qn_ids, context_ids, ans_span, ans_tokens) triples\n",
    "    context_line, qn_line, ans_line = context_file.readline(), qn_file.readline(), ans_file.readline()  # read the next line from each\n",
    "    # print(context_line,qn_line,ans_line)\n",
    "    while context_line and qn_line and ans_line:  # while you haven't reached the end\n",
    "\n",
    "        # Reorganize the map to make the nodes containing the start point comes at the front.\n",
    "        context_line = reorganize(context_line, ans_line)\n",
    "        # Convert tokens to word ids\n",
    "        context_tokens, context_ids = sentence_to_token_ids(context_line, context2id)\n",
    "        qn_tokens, qn_ids = sentence_to_token_ids(qn_line, word2id, is_instr=True)\n",
    "\n",
    "        ans_tokens, ans_ids = sentence_to_token_ids(ans_line, ans2id)\n",
    "\n",
    "        ############# reorganize ans tokens into [start] + [action list] (+ [end]) #####################\n",
    "        if output_goal:\n",
    "            ans_tokens = [ans_tokens[0]] + ans_tokens[1::2] + [ans_tokens[-1]]\n",
    "            ans_ids = [ans_ids[0]] + ans_ids[1::2] + [ans_ids[-1]]\n",
    "        else:\n",
    "            ans_tokens = [ans_tokens[0]] + ans_tokens[1::2]\n",
    "            ans_ids = [ans_ids[0]] + ans_ids[1::2]\n",
    "        ##############################################################################################s\n",
    "        \n",
    "\n",
    "        # read the next line from each file\n",
    "        context_line, qn_line, ans_line = context_file.readline(), qn_file.readline(), ans_file.readline()\n",
    "\n",
    "        # discard or truncate too-long questions\n",
    "        if len(qn_ids) > question_len:\n",
    "            if discard_long:\n",
    "                continue\n",
    "            else:  # truncate\n",
    "                qn_ids = qn_ids[:question_len]\n",
    "\n",
    "        # discard or truncate too-long contexts\n",
    "        if len(context_ids) > context_len:\n",
    "            if discard_long:\n",
    "                continue\n",
    "            else:  # truncate\n",
    "                context_ids = context_ids[:context_len]\n",
    "\n",
    "        # discard or truncate too-long answer\n",
    "        if len(ans_ids) > ans_len:\n",
    "            if discard_long:\n",
    "                continue\n",
    "            else:  # truncate\n",
    "                ans_ids = ans_ids[:ans_len]\n",
    "\n",
    "        # add to examples\n",
    "        examples.append((context_ids, context_tokens, qn_ids, qn_tokens, ans_ids, ans_tokens))\n",
    "\n",
    "        # stop refilling if you have 160 batches\n",
    "        if len(examples) == batch_size * 160:\n",
    "            break\n",
    "\n",
    "    # Once you've either got 160 batches or you've reached end of file:\n",
    "\n",
    "    # Sort by context length for speed\n",
    "    # Note: if you sort by context length, then you'll have batches which contain the same context many times\n",
    "    # (because each context appears several times, with different questions)\n",
    "    # shuffle==False means to not change the sequence of the input data, thus no sorting.\n",
    "    if shuffle:\n",
    "        examples = sorted(examples, key=lambda e: len(e[0]))\n",
    "\n",
    "    # Make into batches and append to the list batches\n",
    "    for batch_start in range(0, len(examples), batch_size):\n",
    "        # Note: each of these is a list length batch_size of lists of ints (except on last iter when it might be less than batch_size)\n",
    "        context_ids_batch, context_tokens_batch, qn_ids_batch, qn_tokens_batch, ans_span_batch, ans_tokens_batch = zip(*examples[batch_start:batch_start + batch_size])\n",
    "\n",
    "        batches.append(\n",
    "            (context_ids_batch, context_tokens_batch, qn_ids_batch, qn_tokens_batch, ans_span_batch, ans_tokens_batch))\n",
    "    if shuffle:\n",
    "        # shuffle the batches\n",
    "        random.shuffle(batches)\n",
    "\n",
    "    toc = time.time()\n",
    "    print(\"Refilling batches took %.2f seconds\" % (toc - tic))\n",
    "    return\n",
    "\n",
    "\n",
    "def get_batch_generator(word2id, context2id, ans2id, context_path, qn_path, ans_path, batch_size, graph_vocab_class,\n",
    "                        context_len, question_len, answer_len, discard_long, shuffle=True, output_goal=False):\n",
    "    \n",
    "    context_file, qn_file, ans_file = open(context_path, encoding=\"utf-8\"), open(qn_path, encoding=\"utf-8\"), open(ans_path, encoding=\"utf-8\")\n",
    "    batches = []\n",
    "\n",
    "\n",
    "    while True:\n",
    "        if len(batches) == 0:  # add more batches\n",
    "            refill_batches(batches, word2id, context2id, ans2id, context_file, qn_file, ans_file, batch_size,\n",
    "                           context_len, question_len, answer_len, discard_long, shuffle=shuffle, output_goal=output_goal)\n",
    "        if len(batches) == 0:\n",
    "            break\n",
    "\n",
    "        # Get next batch. These are all lists length batch_size\n",
    "        (context_ids, context_tokens, qn_ids, qn_tokens, ans_ids, ans_tokens) = batches.pop(0)\n",
    "\n",
    "        # Pad context_ids and qn_ids\n",
    "        qn_ids = padded(qn_ids, question_len)  # pad questions to length question_len\n",
    "        context_ids = padded(context_ids, context_len)  # pad contexts to length context_len\n",
    "        ans_ids = padded(ans_ids, answer_len) # pad ans to maximum length\n",
    "\n",
    "        # Make qn_ids into a np array and create qn_mask\n",
    "        qn_ids = np.array(list(qn_ids))  # shape (batch_size, question_len)\n",
    "        qn_mask = (qn_ids != PAD_ID).astype(np.int32)  # shape (batch_size, question_len)\n",
    "\n",
    "        # Make context_ids into a np array and create context_mask\n",
    "        context_ids = np.array(list(context_ids))  # shape (batch_size, context_len)\n",
    "        # context_mask = (context_ids != PAD_ID).astype(np.int32)  # shape (batch_size, context_len)\n",
    "\n",
    "        # Make ans_ids into a np array and create ans_mask\n",
    "        ans_ids = np.array(list(ans_ids))\n",
    "        ans_mask = (ans_ids != PAD_ID).astype(np.int32)\n",
    "        # print(list(ans_ids), list(context_ids), list(qn_ids))\n",
    "        # interpret graph as triplets and append the first token\n",
    "        # if not show_start_tokens:\n",
    "        # context_embeddings, context_mask = compute_graph_embedding(context_tokens, graph_vocab_class, context_mask.shape[1])\n",
    "        # else:\n",
    "        #     context_embeddings, context_mask = compute_graph_embedding(context_tokens, graph_vocab_class, context_mask.shape[1],\n",
    "        #                                                     np.array([ans_token[0] for ans_token in ans_tokens]))\n",
    "        \n",
    "        # Make into a Batch object\n",
    "        batch = Batch(context_ids, context_tokens, qn_ids, qn_mask, qn_tokens, ans_ids, ans_mask, ans_tokens, batch_size)\n",
    "        # print(len(batch))\n",
    "        yield batch\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Model\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,hidden_size,embedding_size,keep_prob):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.keep_prob = keep_prob\n",
    "        self.gru = nn.GRU(embedding_size,hidden_size,dropout=1-keep_prob,bidirectional=True)\n",
    "    \n",
    "    def forward(self, embedded_inputs):\n",
    "        # print(embedded_inputs.shape,'\\n',embedded_inputs)\n",
    "        # embedded_inputs.to(device)\n",
    "        output,hidden = self.gru(embedded_inputs.to(device))\n",
    "        \n",
    "        return output\n",
    "        # output = [seq_len,batch_size,hidden_size*2]\n",
    "        # hidden = [2,batch_size,hidden_size]\n",
    "        \n",
    "        # can add Dropout layer\n",
    "\n",
    "# class Decoder(nn.Module):\n",
    "#     def __init__(self, batch_size, hidden_size, tgt_vocab_size, max_decoder_length, embeddings, \n",
    "#                 keep_prob, sampling_prob, schedule_embed=False, pred_method='greedy'):\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.projection_layer = nn.Linear(hidden_size,tgt_vocab_size)\n",
    "#         self.gru = nn.GRU(hidden_size,hidden_size)\n",
    "#         self.batch_size = batch_size\n",
    "#         self.embeddings = embeddings\n",
    "#         self.start_id = SOS_ID\n",
    "#         self.end_id = PAD_ID\n",
    "#         self.tgt_vocab_size = tgt_vocab_size\n",
    "#         self.max_decoder_length = max_decoder_length\n",
    "#         self.keep_prob = keep_prob\n",
    "#         self.schedule_embed = schedule_embed\n",
    "#         self.pred_method = pred_method\n",
    "#         self.beam_width = 9\n",
    "#         self.sampling_prob = sampling_prob\n",
    "\n",
    "#     def forward(self, blended_reps_final, encoder_hidden, decoder_emb_inputs, ans_masks, ans_ids, context_masks):\n",
    "#         start_ids = ans_ids[:,0]\n",
    "#         train_output = blended_reps_final\n",
    "#         context_lengths = torch.Tensor([context_masks.size(1)]*self.batch_size)\n",
    "#         decoder_lengths = torch.Tensor([context_masks.size(1)]*self.batch_size)\n",
    "\n",
    "#         # traininghelper vali lines\n",
    "\n",
    "#         pred_start_ids = ans_ids[:,0]\n",
    "\n",
    "#         # pred_helper vali lines\n",
    "\n",
    "\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        stdv = 1. / math.sqrt(self.v.size(0))\n",
    "        self.v.data.normal_(mean=0, std=stdv)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs, src_len=None):\n",
    "        '''\n",
    "        :param hidden: \n",
    "            previous hidden state of the decoder, in shape (layers*directions,B,H)\n",
    "        :param encoder_outputs:\n",
    "            encoder outputs from Encoder, in shape (T,B,H)\n",
    "        :param src_len:\n",
    "            used for masking. NoneType or tensor in shape (B) indicating sequence length\n",
    "        :return\n",
    "            attention energies in shape (B,T)\n",
    "        '''\n",
    "        encoder_outputs = encoder_outputs.transpose(0,1).to(device) # [T*B*H]\n",
    "        max_len = encoder_outputs.size(0)\n",
    "        this_batch_size = encoder_outputs.size(1)\n",
    "        H = hidden.repeat(max_len,1,1).transpose(0,1).to(device)\n",
    "        # print(hidden.shape, encoder_outputs.shape,H.shape)\n",
    "        attn_energies = self.score(H,encoder_outputs) # compute attention score\n",
    "        \n",
    "        if src_len is not None:\n",
    "            mask = []\n",
    "            for b in range(src_len.size(0)):\n",
    "                mask.append([0] * src_len[b].item() + [1] * (encoder_outputs.size(1) - src_len[b].item()))\n",
    "            mask = cuda_(torch.ByteTensor(mask).unsqueeze(1)) # [B,1,T]\n",
    "            attn_energies = attn_energies.masked_fill(mask, -1e18)\n",
    "        \n",
    "        return F.softmax(attn_energies).unsqueeze(1) # normalize with softmax\n",
    "\n",
    "    def score(self, hidden, encoder_outputs):\n",
    "        encoder_outputs = encoder_outputs.transpose(0,1)\n",
    "        # print(hidden.shape,encoder_outputs.shape)\n",
    "\n",
    "        energy = F.tanh(self.attn(torch.cat([hidden, encoder_outputs], 2))) # [B*T*2H]->[B*T*H]\n",
    "        energy = energy.transpose(2,1) # [B*H*T]\n",
    "        v = self.v.repeat(encoder_outputs.data.shape[0],1).unsqueeze(1) #[B*1*H]\n",
    "        energy = torch.bmm(v,energy) # [B*1*T]\n",
    "        return energy.squeeze(1) #[B*T]\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embed_size, output_size, n_layers=1, dropout_p=0.1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        # Define parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_size = embed_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        # Define layers\n",
    "        self.embedding_dec = nn.Embedding(output_size, embed_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.attn = Attn('concat', hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size + embed_size, hidden_size, n_layers, dropout=dropout_p)\n",
    "        #self.attn_combine = nn.Linear(hidden_size + embed_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, word_input, last_hidden, encoder_outputs):\n",
    "        '''\n",
    "        :param word_input: === decoder_input\n",
    "            word input for current time step, in shape (B)\n",
    "        :param last_hidden:=== decoder_hidden\n",
    "            last hidden stat of the decoder, in shape (layers*direction*B*H)\n",
    "        :param encoder_outputs:\n",
    "            encoder outputs in shape (T*B*H)\n",
    "        :return\n",
    "            decoder output\n",
    "        Note: we run this one step at a time i.e. you should use a outer loop \n",
    "            to process the whole sequence\n",
    "            '''\n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        word_input = word_input.long().to(device)\n",
    "        last_hidden = last_hidden.to(device)\n",
    "        encoder_outputs = encoder_outputs.to(device)\n",
    "\n",
    "        word_embedded = self.embedding_dec(word_input).view(1, word_input.size(0), -1) # (1,B,V)\n",
    "        word_embedded = self.dropout(word_embedded)\n",
    "        # Calculate attention weights and apply to encoder outputs\n",
    "        attn_weights = self.attn(last_hidden[-1], encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs)  # (B,1,V)\n",
    "        context = context.transpose(0, 1)  # (1,B,V)\n",
    "        # Combine embedded input word and attended context, run through RNN\n",
    "        # print(word_embedded.shape,context.shape)\n",
    "        # word_embedded = [1,1,100]\n",
    "        # context = [1,1,128]\n",
    "        rnn_input = torch.cat((word_embedded, context), 2)\n",
    "        #rnn_input = self.attn_combine(rnn_input) # use it in case your size of rnn_input is different\n",
    "        # rnn_input = [1,1,228]\n",
    "        # print(last_hidden.shape)\n",
    "        # last_hidden = [150,128]\n",
    "        output, hidden = self.gru(rnn_input, last_hidden)\n",
    "        output = output.squeeze(0)  # (1,B,V)->(B,V)\n",
    "        # context = context.squeeze(0)\n",
    "        # update: \"context\" input before final layer can be problematic.\n",
    "        # output = F.log_softmax(self.out(torch.cat((output, context), 1)))\n",
    "        output = F.log_softmax(self.out(output))\n",
    "        # Return final output, hidden state\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "\n",
    "class BasicAttn(nn.Module):\n",
    "    def __init__(self, keep_prob, key_vec_size, value_vec_size):\n",
    "        super(BasicAttn,self).__init__()\n",
    "        self.keep_prob = keep_prob\n",
    "        self.key_vec_size = key_vec_size\n",
    "        self.value_vec_size = value_vec_size\n",
    "\n",
    "    def forward(self, values, values_mask, keys):\n",
    "        # values = torch.from_numpy(values).float() \n",
    "        values_mask = torch.from_numpy(values_mask).float().to(device) \n",
    "        # keys = torch.from_numpy(keys).float() \n",
    "        # values = values.to(device)\n",
    "        # keys = keys.to(device)\n",
    "        attn_logits_mask = torch.unsqueeze(values_mask, 1).to(device) # -> (batch_size, 1, num_values)\n",
    "        \n",
    "        w = torch.zeros(self.key_vec_size, self.value_vec_size)\n",
    "        w = nn.init.xavier_normal_(w).to(device)\n",
    "        values_t = torch.transpose(values, 0, 1) \n",
    "        values_t = torch.transpose(values_t, 1,2)# -> (batch_size, value_vec_size, num_values)\n",
    "        def fn(a, x):\n",
    "            return torch.matmul(x, w)\n",
    "\n",
    "        list_ = [fn(8, keys[i, :,:]) for i in range(keys.shape[0])]\n",
    "        part_logits = torch.stack(list_)\n",
    "        # part_logits = torch.Tensor(list_) # (batch_size, num_keys, value_vec)\n",
    "        # print(values_t.shape)\n",
    "        attn_logits = torch.bmm(part_logits, values_t).to(device) # -> (batch_size, num_keys, num_values)\n",
    "        _, attn_dist = masked_softmax(attn_logits, attn_logits_mask, dim = -1)\n",
    "        # attn_dist = attn_dist.transpose(1,2)\n",
    "        # print('attn dist: ',attn_dist.shape)\n",
    "        # print(values.shape,values_t.shape)\n",
    "        output = torch.matmul(attn_dist, values.transpose(0,1))\n",
    "\n",
    "        return attn_dist, output\n",
    "\n",
    "\n",
    "class QAModel(nn.Module):\n",
    "    def __init__(self, id2word, word2id, emb_matrix, ans2id, id2ans, context2id,hidden_size, embedding_size, tgt_vocab_size,batch_size):\n",
    "        super(QAModel,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.tgt_vocab_size = tgt_vocab_size\n",
    "        self.id2word = id2word\n",
    "        self.word2id = word2id\n",
    "        self.ans_vocab_size = len(ans2id)\n",
    "        self.ans2id = ans2id\n",
    "        self.id2ans = id2ans\n",
    "        self.batch_size = batch_size\n",
    "        self.emb_matrix = emb_matrix\n",
    "        self.context2id = context2id\n",
    "        self.keep_prob = 0.8\n",
    "        self.embedding = nn.Embedding(len(context2id),embedding_size)\n",
    "        self.linear21 = nn.Linear(2*hidden_size,hidden_size)\n",
    "        self.linear41 = nn.Linear(4*hidden_size,hidden_size)\n",
    "        self.graph_vocab_class = create_vocab_class(context2id)\n",
    "        self.context_dimension_compressed = len(self.graph_vocab_class.all_tokens) + len(self.graph_vocab_class.nodes)\n",
    "\n",
    "        self.encoder = Encoder(self.hidden_size,self.embedding_size, self.keep_prob).to(device)\n",
    "        self.decoder = DecoderRNN(hidden_size,embedding_size,tgt_vocab_size).to(device)\n",
    "\n",
    "    def forward(self,qn_ids,context_ids,ans_ids,qn_mask):\n",
    "\n",
    "        context_embs = self.embedding(context_ids)\n",
    "        # context_embs.to(device)\n",
    "        context_hiddens = self.encoder(context_embs)  # (batch_size, context_len, hidden_size*2)\n",
    "\n",
    "        qn_embs = self.get_embeddings(self.id2word,self.emb_matrix,qn_ids,self.embedding_size, self.batch_size)\n",
    "        question_hiddens = self.encoder(qn_embs)  # (batch_size, question_len, hidden_size*2)\n",
    "        # print('question hiddens: ',question_hiddens.shape)\n",
    "        question_last_hidden = question_hiddens[-1, :, :]\n",
    "        # question_last_hidden.to(device)\n",
    "        # print('question last hidden: ',question_last_hidden.shape)\n",
    "        question_last_hidden = self.linear21(question_last_hidden)\n",
    "        question_last_hidden = question_last_hidden.unsqueeze(0)\n",
    "        # Working fine till here\n",
    "\n",
    "        attn_layer = BasicAttn(self.keep_prob, self.hidden_size * 2, self.hidden_size * 2)\n",
    "        _, attn_output = attn_layer(question_hiddens, qn_mask, context_hiddens)\n",
    "        # Concat attn_output to context_hiddens to get blended_reps\n",
    "        blended_reps = torch.cat((context_hiddens, attn_output), axis=2)  # (batch_size, context_len, hidden_size*4)\n",
    "        blended_reps_final = self.linear41(blended_reps)\n",
    "        dec_hidden = question_last_hidden\n",
    "        # Idhar for loop lagaane ka hai\n",
    "        decoder_outputs = []\n",
    "        # print('ans_ids',len(ans_ids))\n",
    "        ans_ids = torch.tensor(ans_ids)\n",
    "        ans_ids = ans_ids.transpose(0,1)\n",
    "        # print(ans_ids.shape)\n",
    "        tgt_len,_ = ans_ids.shape\n",
    "        # ans_ids = [50=tgt_len,bsz]\n",
    "        for idx in range(tgt_len):\n",
    "            dec_output,dec_hidden = self.decoder(ans_ids[idx,:],dec_hidden,blended_reps_final)\n",
    "            decoder_outputs.append(dec_output)\n",
    "            # topk\n",
    "            # loss add\n",
    "        return decoder_outputs #, loss\n",
    "        \n",
    "        # ----------------------------------- #\n",
    "        \n",
    "    def get_embeddings(self,token2id,embed_matrix,input_ids,embed_size,batch_size):\n",
    "            array = np.zeros((len(input_ids[0]),batch_size,embed_size)) \n",
    "            # input_ids = [bsz,src_len]\n",
    "            # print(input_ids)\n",
    "            for idx,tokenised_words in enumerate(input_ids):\n",
    "                # words = [token2id[char_id] for char_id in tokenised_id]\n",
    "                for word_idx,word in enumerate(tokenised_words):\n",
    "                    array[word_idx,idx,:] = embed_matrix[word,:]\n",
    "            vector = torch.from_numpy(array).float()\n",
    "            # print(vector.size,vector)\n",
    "            return vector.to(device)\n",
    "\n",
    "\n",
    "\n",
    "def masked_softmax(logits, masks, dim):\n",
    "    # print('attn logits: ',logits.shape)\n",
    "    inf_mask = (1 - masks.type(torch.FloatTensor)) * (-1e30)\n",
    "    inf_mask = inf_mask.to(device)\n",
    "    masked_logits = torch.add(logits, inf_mask)\n",
    "    sm = nn.LogSoftmax(dim)\n",
    "    softmax_out = sm(masked_logits)\n",
    "    return masked_logits.to(device), softmax_out.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/18110188/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.19999999999999996 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "/home/18110188/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refilling batches...\n",
      "Refilling batches took 11.97 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/18110188/.local/lib/python3.6/site-packages/torch/nn/functional.py:1339: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/home/18110188/.local/lib/python3.6/site-packages/ipykernel_launcher.py:97: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/18110188/.local/lib/python3.6/site-packages/ipykernel_launcher.py:164: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of 50 batches with loss = 2.8000874519348145\n",
      "End of 100 batches with loss = 2.5970027446746826\n",
      "End of 150 batches with loss = 2.5395398139953613\n",
      "Refilling batches...\n",
      "Refilling batches took 8.96 seconds\n",
      "End of 200 batches with loss = 2.315581798553467\n",
      "End of 250 batches with loss = 2.7311747074127197\n",
      "End of 300 batches with loss = 2.550208330154419\n",
      "Refilling batches...\n",
      "Refilling batches took 8.65 seconds\n",
      "End of 350 batches with loss = 2.594959020614624\n",
      "End of 400 batches with loss = 2.375993013381958\n",
      "End of 450 batches with loss = 2.577575206756592\n",
      "Refilling batches...\n",
      "Refilling batches took 1.41 seconds\n",
      "invalid argument 7: equal number of batches expected at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:493\n",
      "End of 500 batches with loss = 2.8143985271453857\n",
      "Refilling batches...\n",
      "Refilling batches took 0.00 seconds\n",
      "End of epoch 1  | Loss =  1516.9305906295776\n",
      "296.8049328327179\n",
      "Refilling batches...\n",
      "Refilling batches took 8.44 seconds\n",
      "End of 50 batches with loss = 2.5820424556732178\n",
      "End of 100 batches with loss = 2.5455665588378906\n",
      "End of 150 batches with loss = 2.285659074783325\n",
      "Refilling batches...\n",
      "Refilling batches took 9.14 seconds\n",
      "End of 200 batches with loss = 2.2720634937286377\n",
      "End of 250 batches with loss = 2.3538577556610107\n",
      "End of 300 batches with loss = 2.5034258365631104\n",
      "Refilling batches...\n",
      "Refilling batches took 8.67 seconds\n",
      "End of 350 batches with loss = 2.436863899230957\n",
      "End of 400 batches with loss = 2.7741870880126953\n",
      "End of 450 batches with loss = 2.394604206085205\n",
      "Refilling batches...\n",
      "Refilling batches took 1.42 seconds\n",
      "invalid argument 7: equal number of batches expected at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:493\n",
      "End of 500 batches with loss = 2.242917060852051\n",
      "Refilling batches...\n",
      "Refilling batches took 0.00 seconds\n",
      "End of epoch 2  | Loss =  1263.9299875497818\n",
      "296.0844600200653\n",
      "Refilling batches...\n",
      "Refilling batches took 8.45 seconds\n",
      "End of 50 batches with loss = 2.3331451416015625\n",
      "End of 100 batches with loss = 2.193970203399658\n",
      "End of 150 batches with loss = 2.333332061767578\n",
      "Refilling batches...\n",
      "Refilling batches took 8.81 seconds\n",
      "End of 200 batches with loss = 2.853940963745117\n",
      "End of 250 batches with loss = 2.0943479537963867\n",
      "End of 300 batches with loss = 2.736100435256958\n",
      "Refilling batches...\n",
      "Refilling batches took 8.75 seconds\n",
      "End of 350 batches with loss = 2.3469083309173584\n",
      "End of 400 batches with loss = 2.623750686645508\n",
      "End of 450 batches with loss = 2.336005687713623\n",
      "Refilling batches...\n",
      "Refilling batches took 1.69 seconds\n",
      "invalid argument 7: equal number of batches expected at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:493\n",
      "End of 500 batches with loss = 2.4202871322631836\n",
      "Refilling batches...\n",
      "Refilling batches took 0.00 seconds\n",
      "End of epoch 3  | Loss =  1245.4224914312363\n",
      "296.91494822502136\n"
     ]
    }
   ],
   "source": [
    "qa_model = QAModel(id2word, word2id, emb_matrix, context_vocab, rev_context_vocab, context_vocab, HIDDEN_SIZE, EMBEDDING_SIZE, NO_CLASS,BATCH_SIZE)\n",
    "qa_model = qa_model.to(device)\n",
    "# file_handler = logging.FileHandler(os.path.join(\"./model\", \"log.txt\"))\n",
    "# logging.getLogger().addHandler(file_handler)\n",
    "\n",
    "# if LOAD_PREV:\n",
    "#     qa_model.load_state_dict(torch.load(\"./data\"))\n",
    "\n",
    "train(qa_model, NUM_EPOCH, \"./data/train.graph\", \"./data/train.instruction\", \"./data/train.answer\", BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {'model': QAModel(id2word, word2id, emb_matrix, context_vocab, rev_context_vocab, context_vocab, HIDDEN_SIZE, EMBEDDING_SIZE, NO_CLASS,BATCH_SIZE).to(device),\n",
    "              'state_dict': qa_model.state_dict(),\n",
    "              \"encoder_optimizer\": optim.Adam(qa_model.encoder.parameters(),lr = 0.001).state_dict(),\n",
    "              \"decoder_optimizer\": optim.Adam(qa_model.decoder.parameters(),lr = 0.001).state_dict()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/18110188/.local/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type QAModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/18110188/.local/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/18110188/.local/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type DecoderRNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/18110188/.local/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Attn. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(checkpoint, 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import string\n",
    "import re\n",
    "import argparse\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Interface for summarizing all metrics\n",
    "# input are two lists of strings of form [start] + [action list] + [goal]\n",
    "def compute_all_metrics(pred_answer, true_answer):\n",
    "    # because we only consider the accuracy of the actions, we remove first and last items.\n",
    "    prediction_str = normalize_answer(\" \".join(pred_answer[1:-1]))\n",
    "    ground_truth_str = normalize_answer(\" \".join(true_answer[1:-1]))\n",
    "    em = exact_match_score(prediction_str, ground_truth_str)\n",
    "    f1 = f1_score(prediction_str, ground_truth_str)\n",
    "    ed = edit_distance(prediction_str.split(), ground_truth_str.split())\n",
    "    if em > int(pred_answer[-1] == true_answer[-1]):\n",
    "        print(\"weird thing happens, pred {}, true {}\".format(pred_answer, true_answer))\n",
    "    return f1, em, ed, pred_answer[-1] == true_answer[-1]\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = prediction.split()\n",
    "    ground_truth_tokens = ground_truth.split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "def edit_distance(s1, s2):\n",
    "    \"\"\"\n",
    "    :param s1: list\n",
    "    :param s2: list\n",
    "    :return: edit distance of two lists\n",
    "    \"\"\"\n",
    "    if len(s1) < len(s2):\n",
    "        return edit_distance(s2, s1)\n",
    "\n",
    "    # len(s1) >= len(s2)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "\n",
    "    previous_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[\n",
    "                             j + 1] + 1  # j+1 instead of j since previous_row and current_row are one character longer\n",
    "            deletions = current_row[j] + 1  # than s2\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "\n",
    "    return previous_row[-1]\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return prediction == ground_truth\n",
    "\n",
    "def rough_match_score(prediction, ground_truth):\n",
    "    prediction = ' '.join(prediction.split(' '))\n",
    "    ground_truth = ' '.join(ground_truth.split(' '))\n",
    "    pred_list = prediction.split(' ')\n",
    "    truth_list = ground_truth.split(' ')\n",
    "    poss_correct = len(pred_list) == len(truth_list) or \\\n",
    "                   (len(pred_list) > len(truth_list) and pred_list[len(truth_list)] not in ['oor', 'ool'])\n",
    "    return prediction[: len(ground_truth)] == ground_truth and poss_correct\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "    return max(scores_for_ground_truths)\n",
    "\n",
    "\n",
    "def evaluate(ground_truth, predictions):\n",
    "    f1_total = em_total = 0\n",
    "    total = len(ground_truth)\n",
    "    err_analysis = defaultdict(list)\n",
    "    assert len(ground_truth) == len(predictions)\n",
    "\n",
    "    for i in range(total):\n",
    "        truth = ground_truth[i].strip().split(' ')[1:]\n",
    "        pred = predictions[i].strip().split(' ')\n",
    "        f1 = f1_score(predictions[i], \" \".join(truth))\n",
    "        em = exact_match_score(predictions[i], \" \".join(truth))\n",
    "        for j in range(len(truth)):\n",
    "            err_analysis[j].append(j < len(pred) and truth[j] == pred[j])\n",
    "        f1_total += f1\n",
    "        em_total += em\n",
    "    err_dist = np.zeros([len(err_analysis)])\n",
    "\n",
    "    for k in err_analysis:\n",
    "        err_dist[k] = sum(err_analysis[k]) / float(len(err_analysis[k]))\n",
    "    plt.plot(err_dist)\n",
    "    plt.xlabel(\"pos in the answer\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.show()\n",
    "    exact_match = 100.0 * em_total / total\n",
    "    f1 = 100.0 * f1_total / total\n",
    "    print('exact_match: {}, f1: {}'.format(exact_match, f1))\n",
    "    return\n",
    "\n",
    "def evaluate_new(ground_truth, predictions):\n",
    "    \"\"\"\n",
    "    :param ground_truth: a list of strings\n",
    "    :param predictions: a list of strings\n",
    "    :return: nil, side effect: print out the metrics value.\n",
    "    \"\"\"\n",
    "    assert len(ground_truth) == len(predictions)\n",
    "    f1_all = 0.0\n",
    "    em_all = 0.0\n",
    "    ed_all = 0.0\n",
    "    gem_all = 0.0\n",
    "    i = 0\n",
    "    for (g, p) in zip(ground_truth, predictions):\n",
    "        i += 1\n",
    "        # print(i)\n",
    "        pred_answer = p.strip().split(\" \")\n",
    "        true_answer = g.strip().split(\" \")\n",
    "        true_answer = [true_answer[0]] + true_answer[1::2] + [true_answer[-1]]\n",
    "        f1, em, ed, gem = compute_all_metrics(pred_answer, true_answer)\n",
    "        f1_all += f1\n",
    "        em_all += em\n",
    "        ed_all += ed\n",
    "        gem_all += gem\n",
    "\n",
    "    f1_all /= len(ground_truth)\n",
    "    em_all /= len(ground_truth)\n",
    "    ed_all /= len(ground_truth)\n",
    "    gem_all /= len(ground_truth)\n",
    "    print(\"f1 {}, em {}, ed {}, gem {}\".format(f1_all, em_all, ed_all, gem_all))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/train.answer\") as true_file:\n",
    "    dataset = true_file.readlines()\n",
    "with open(args.prediction_file) as prediction_file:\n",
    "    predictions = prediction_file.readlines()\n",
    "evaluate_new(dataset, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
