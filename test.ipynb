{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Training Steps\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "def loss(loss_func,logits,target):\n",
    "    return loss_func(logits,target)\n",
    "\n",
    "\n",
    "\n",
    "def train_iteration(qamodel,batch,criterion,encoder_optimizer,decoder_optimizer):\n",
    "    context_ids = batch.context_ids\n",
    "    qn_ids = batch.qn_ids\n",
    "    ans_ids = batch.ans_ids\n",
    "    qn_mask = batch.qn_mask\n",
    "    batch_size = batch.batch_size\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    context_ids = torch.from_numpy(context_ids)\n",
    "    decoder_outputs= qamodel(qn_ids,context_ids,ans_ids,qn_mask)\n",
    "    loss = 0    #loss per batch\n",
    "    for idx,dec_out in enumerate(decoder_outputs):\n",
    "        loss += criterion(dec_out,ans_ids[idx])\n",
    "    \n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item()/batch_size\n",
    "\n",
    "\n",
    "def train(qamodel,num_epochs, context_path, qn_path, ans_path, batch_size):\n",
    "    epoch = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    encoder_optimizer = optim.Adam(qamodel.encoder.parameters(),lr = 0.001)\n",
    "    decoder_optimizer = optim.Adam(qamodel.decoder.parameters(),lr = 0.001)\n",
    "    # initialise optimiser\n",
    "    while epoch<num_epochs:\n",
    "        epoch+=1\n",
    "        epoch_loss = 0\n",
    "        epoch_start_time = time.time()\n",
    "        num_iters = 0\n",
    "        for batch in get_batch_generator(qamodel.word2id, qamodel.context2id, qamodel.ans2id, context_path,\n",
    "                                            qn_path, ans_path, batch_size, qamodel.graph_vocab_class,\n",
    "                                            context_len=300, question_len=150,\n",
    "                                            answer_len=50, discard_long=False):\n",
    "            batch_loss  = train_iteration(qamodel,batch,criterion,encoder_optimizer,decoder_optimizer)\n",
    "            num_iters += 1\n",
    "            epoch_loss += batch_loss\n",
    "        \n",
    "            # loss backward\n",
    "            # optimiser step\n",
    "            # if num_iters%print_every:\n",
    "\n",
    "            # add line to print at print_every\n",
    "        epoch_end_time = time.time()\n",
    "        time_of_epoch = epoch_end_time - epoch_start_time\n",
    "        print(time_of_epoch)\n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab.py\n",
    "\n",
    "\"\"\"This file contains a function to read the GloVe vectors from file,\n",
    "and return them as an embedding matrix\"\"\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "_PAD = r\"<pad>\"\n",
    "_UNK = r\"<unk>\"\n",
    "_SOS = r\"<sos>\"\n",
    "_START_VOCAB = [_PAD, _UNK, _SOS]\n",
    "PAD_ID = 0\n",
    "UNK_ID = 1\n",
    "SOS_ID = 2\n",
    "\n",
    "# Regular expressions used to tokenize.\n",
    "_WORD_SPLIT = re.compile(r\"([.,!?\\\"':;()/-])\")\n",
    "_DIGIT_RE = re.compile(r\"\\d\")\n",
    "_ATTRIBUTE_RE = r'\\d+[A-Za-z]+'\n",
    "_NODE_RE = r'^[O|R|K|B|C|H|L]-\\d+'\n",
    "_ACT_RE = r'[A-Za-z]+'\n",
    "_DISCARD_TOK = ['(', ')', 'nt', ';']\n",
    "\n",
    "\n",
    "class Vocab:\n",
    "    \"\"\"\n",
    "    Read in the class Vocab, it is a tidied class containing classified elements in graph\n",
    "    self.node2id: dictionary converting the node token to its id\n",
    "    self.nodes: list of all node tokens\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, node2id, edge2id, flag2id):\n",
    "        self.discard_tokens = _DISCARD_TOK\n",
    "        self.node2id = node2id\n",
    "        self.edge2id = edge2id\n",
    "        self.flag2id = flag2id\n",
    "        self.nodes = list(node2id.keys())\n",
    "        self.edges = list(edge2id.keys())\n",
    "        self.flags = list(flag2id.keys())\n",
    "        self.all_tokens = self.flags + self.nodes + self.edges\n",
    "\n",
    "    def tidy_in_triplet(self, tokens):\n",
    "        \"\"\"\n",
    "        convert raw tokens into a id list of length [3*N]\n",
    "        [first_node_id_list, edge_id_list, second_id_list, first_id_list, edge_id_list, second_id_list, ...]\n",
    "        each entry in the list contains another list because node can be composed of many elements\n",
    "        [[node_element0, node_element1, ...], [edge_element0, edge_element1, ...], [...], ...]\n",
    "        :param tokens: a list of raw tokens in graph txt file\n",
    "        :return: a list of lists\n",
    "        \"\"\"\n",
    "        ids = []\n",
    "        for (i, w) in enumerate(tokens):\n",
    "            if w in self.nodes:\n",
    "                if (i == 0) or (tokens[i - 1] in [';', ')']) or (tokens[i - 1] in self.edges):\n",
    "                    ids.append([self.node2id[w]])\n",
    "                else:\n",
    "                    ids[-1].append(self.node2id[w])\n",
    "            elif w in self.edges:\n",
    "                if (tokens[i - 1] in self.flags) or (tokens[i - 1] in self.nodes):\n",
    "                    ids.append([self.edge2id[w]])\n",
    "                else:\n",
    "                    ids[-1].append(self.edge2id[w])\n",
    "            elif w in ['l', 'r']:\n",
    "                w = \"-\".join(tokens[i - 1: i + 1])\n",
    "                if w in self.edges:\n",
    "                    ids[-1].append(self.edge2id[w])\n",
    "                else:\n",
    "                    ids[-1].append(UNK_ID)\n",
    "                    \n",
    "            elif (w in self.discard_tokens) or (re.match(_ATTRIBUTE_RE, w)):\n",
    "                if w == ';':\n",
    "                    assert len(ids) % 3 == 0, \"error in tidy_in_triplet, can't be divided by 3\"\n",
    "                continue\n",
    "            elif w not in self.all_tokens:\n",
    "                raise ValueError(\"new token %s in graph representation.\"%w)\n",
    "        return ids\n",
    "\n",
    "def get_glove(glove_path, glove_dim):\n",
    "    \"\"\"Reads from original GloVe .txt file and returns embedding matrix and\n",
    "    mappings from words to word ids.\n",
    "\n",
    "    Input:\n",
    "      glove_path: path to glove.6B.{glove_dim}d.txt\n",
    "      glove_dim: integer; needs to match the dimension in glove_path\n",
    "\n",
    "    Returns:\n",
    "      emb_matrix: Numpy array shape (400002, glove_dim) containing glove embeddings\n",
    "(plus PAD and UNK embeddings in first two rows).\n",
    "        The rows of emb_matrix correspond to the word ids given in word2id and id2word\n",
    "      word2id: dictionary mapping word (string) to word id (int)\n",
    "      id2word: dictionary mapping word id (int) to word (string)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Loading GLoVE vectors from file: %s\" % glove_path)\n",
    "    vocab_size = int(4e5)  # this is the vocab size of the corpus we've downloaded\n",
    "\n",
    "    emb_matrix = np.zeros((vocab_size + len(_START_VOCAB), glove_dim))\n",
    "    word2id = {}\n",
    "    id2word = {}\n",
    "\n",
    "    random_init = True\n",
    "    # randomly initialize the special tokens\n",
    "    if random_init:\n",
    "        emb_matrix[:len(_START_VOCAB), :] = np.random.randn(len(_START_VOCAB), glove_dim)\n",
    "\n",
    "    # put start tokens in the dictionaries\n",
    "    idx = 0\n",
    "    for word in _START_VOCAB:\n",
    "        word2id[word] = idx\n",
    "        id2word[idx] = word\n",
    "        idx += 1\n",
    "\n",
    "    # go through glove vecs\n",
    "    with open(glove_path, 'r', encoding=\"utf-8\") as fh:\n",
    "        for line in tqdm(fh, total=vocab_size):\n",
    "            line = line.lstrip().rstrip().split(\" \")\n",
    "            word = line[0]\n",
    "            vector = list(map(float, line[1:]))\n",
    "            if glove_dim != len(vector):\n",
    "                raise Exception(\n",
    "                    \"You set --glove_path=%s but --embedding_size=%i. If you set --glove_path yourself then make sure that --embedding_size matches!\" % (\n",
    "                    glove_path, glove_dim))\n",
    "            emb_matrix[idx, :] = vector\n",
    "            word2id[word] = idx\n",
    "            id2word[idx] = word\n",
    "            idx += 1\n",
    "\n",
    "    final_vocab_size = vocab_size + len(_START_VOCAB)\n",
    "    assert len(word2id) == final_vocab_size\n",
    "    assert len(id2word) == final_vocab_size\n",
    "    assert idx == final_vocab_size\n",
    "\n",
    "    return emb_matrix, word2id, id2word\n",
    "\n",
    "\n",
    "def one_hot_converter(vec_len):\n",
    "    one_hot_embed = np.zeros((vec_len, vec_len))\n",
    "    np.fill_diagonal(one_hot_embed, 1)\n",
    "    return one_hot_embed\n",
    "\n",
    "def instruction_tokenizer(sentence):\n",
    "    \"\"\"\n",
    "    A special tokenizer for instructions.\n",
    "    Turn into lower case and split Office-1 or office1 into \"Office 1\",\n",
    "    :param sentence: instructions (natural language)\n",
    "    :return: a list of tokens\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    prepocessed_sen_list = preprocess_instruction(sentence.strip())\n",
    "    for space_separated_fragment in prepocessed_sen_list:\n",
    "        words.extend(_WORD_SPLIT.split(space_separated_fragment))\n",
    "    return [w.lower() for w in words if w]\n",
    "\n",
    "def preprocess_instruction(sentence):\n",
    "    # change \"office-12\" or \"office12\" to \"office 12\"\n",
    "    # change \"12-office\" or \"12office\" to \"12 office\"\n",
    "    _WORD_NO_SPACE_NUM_RE = r'([A-Za-z]+)\\-?(\\d+)'\n",
    "    _NUM_NO_SPACE_WORD_RE = r'(\\d+)\\-?([A-Za-z]+)'\n",
    "    new_str = re.sub(_WORD_NO_SPACE_NUM_RE, lambda m: m.group(1) + ' ' + m.group(2), sentence)\n",
    "    new_str = re.sub(_NUM_NO_SPACE_WORD_RE, lambda m: m.group(1) + ' ' + m.group(2), new_str)\n",
    "    lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "    # correct common typos.\n",
    "    correct_error_dic = {'rom': 'room', 'gout': 'go out', 'roo': 'room',\n",
    "                         'immeidately': 'immediately', 'halway': 'hallway',\n",
    "                         'office-o': 'office 0', 'hall-o': 'hall 0', 'pas': 'pass',\n",
    "                         'offic': 'office', 'leftt': 'left', 'iffice': 'office'}\n",
    "    for err_w in correct_error_dic:\n",
    "        find_w = ' ' + err_w + ' '\n",
    "        replace_w = ' ' + correct_error_dic[err_w] + ' '\n",
    "        new_str = new_str.replace(find_w, replace_w)\n",
    "    sen_list = []\n",
    "    # Lemmatize words\n",
    "    for word in new_str.split(' '):\n",
    "        try:\n",
    "            word = lemma.lemmatize(word)\n",
    "            if len(word) > 0 and word[-1] == '-':\n",
    "                word = word[:-1]\n",
    "            if word:\n",
    "                sen_list.append(word)\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "            # print(\"unicode error \", word, new_str)\n",
    "    return sen_list\n",
    "\n",
    "def basic_tokenizer(sentence):\n",
    "    \"\"\"Very basic tokenizer: split the sentence into a list of tokens.\"\"\"\n",
    "    words = []\n",
    "    for space_separated_fragment in sentence.strip().split():\n",
    "        words.extend(_WORD_SPLIT.split(space_separated_fragment))\n",
    "    return [w for w in words if w]\n",
    "\n",
    "def create_vocab_class(vocab_dict):\n",
    "    \"\"\"\n",
    "    Convert the raw tokens in graph to\n",
    "    To organize the vocab into two groups: node group, action + attribute group\n",
    "    :param vocab_dict:\n",
    "    :param rev_vocab:\n",
    "    :return: A vocab class holding all the information necessary for training\n",
    "    \"\"\"\n",
    "    rev_vocab = vocab_dict.keys()\n",
    "    new_vocab_dic = {\"node\": [\"S\", \"N\", \"E\", \"W\"], \"edge\": [], \"flag\": _START_VOCAB}\n",
    "    for vocab in rev_vocab:\n",
    "        if vocab in 'lrSNEW':\n",
    "            continue\n",
    "        elif re.match(_NODE_RE, vocab):\n",
    "            new_vocab_dic[\"node\"].append(vocab)\n",
    "        elif re.match(_ATTRIBUTE_RE, vocab):\n",
    "            new_vocab_dic[\"edge\"].append(vocab + '-l')\n",
    "            new_vocab_dic[\"edge\"].append(vocab + '-r')\n",
    "        elif re.match(_ACT_RE, vocab) and vocab != 'nt':\n",
    "            new_vocab_dic[\"edge\"].append(vocab)\n",
    "    node2id = dict([(x, y) for (y, x) in enumerate(new_vocab_dic['node'])])\n",
    "    edge2id = dict([(x, y) for (y, x) in enumerate(new_vocab_dic['edge'])])\n",
    "    flag2id = dict([(x, y) for (y, x) in enumerate(new_vocab_dic['flag'])])\n",
    "\n",
    "    return Vocab(node2id, edge2id, flag2id)\n",
    "\n",
    "\n",
    "def create_vocabulary(vocabulary_path, data_path, max_vocabulary_size,\n",
    "                      tokenizer=None, normalize_digits=False):\n",
    "    \"\"\"Create vocabulary file (if it does not exist yet) from data file.\n",
    "\n",
    "    Data file is assumed to contain one sentence per line. Each sentence is\n",
    "    tokenized and digits are normalized (if normalize_digits is set).\n",
    "    Vocabulary contains the most-frequent tokens up to max_vocabulary_size.\n",
    "    We write it to vocabulary_path in a one-token-per-line format, so that later\n",
    "    token in the first line gets id=0, second line gets id=1, and so on.\n",
    "\n",
    "    Args:\n",
    "      vocabulary_path: path where the vocabulary will be created.\n",
    "      data_path: data file that will be used to create vocabulary.\n",
    "      max_vocabulary_size: limit on the size of the created vocabulary.\n",
    "      tokenizer: a function to use to tokenize each data sentence;\n",
    "        if None, basic_tokenizer will be used.\n",
    "      normalize_digits: Boolean; if true, all digits are replaced by 0s.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(vocabulary_path):\n",
    "        print(\"Creating vocabulary %s from data %s\" % (vocabulary_path, data_path))\n",
    "        vocab = {}\n",
    "        with open(data_path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            counter = 0\n",
    "            for line in f:\n",
    "                counter += 1\n",
    "                if counter % 100000 == 0:\n",
    "                    print(\"  processing line %d\" % counter)\n",
    "                \n",
    "                # TODO - CHANGE THE BELOW LINE\n",
    "                # line = tf.compat.as_bytes(line)\n",
    "                tokens = tokenizer(line) if tokenizer else basic_tokenizer(line)\n",
    "                for w in tokens:\n",
    "                    word = _DIGIT_RE.sub(r\"0\", w) if normalize_digits else w\n",
    "                    if word in vocab:\n",
    "                        vocab[word] += 1\n",
    "                    else:\n",
    "                        vocab[word] = 1\n",
    "            vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n",
    "            if len(vocab_list) > max_vocabulary_size:\n",
    "                vocab_list = vocab_list[:max_vocabulary_size]\n",
    "            with open(vocabulary_path, mode=\"w\", encoding=\"utf-8\") as vocab_file:\n",
    "                for w in vocab_list:\n",
    "                    vocab_file.write(w + b\"\\n\")\n",
    "        rev_vocab = vocab_list # a list contain all the tokens\n",
    "        vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])  # key is the token, value is index\n",
    "        return vocab, rev_vocab\n",
    "    else:\n",
    "        print(\"Skipping generating vocabulary file for {}\".format(vocabulary_path))\n",
    "        return initialize_vocabulary(vocabulary_path)\n",
    "\n",
    "def initialize_vocabulary(vocabulary_path):\n",
    "  \"\"\"Initialize vocabulary from file.\n",
    "\n",
    "  We assume the vocabulary is stored one-item-per-line, so a file:\n",
    "    dog\n",
    "    cat\n",
    "  will result in a vocabulary {\"dog\": 0, \"cat\": 1}, and this function will\n",
    "  also return the reversed-vocabulary [\"dog\", \"cat\"].\n",
    "\n",
    "  Args:\n",
    "    vocabulary_path: path to the file containing the vocabulary.\n",
    "\n",
    "  Returns:\n",
    "    a pair: the vocabulary (a dictionary mapping string to integers), and\n",
    "    the reversed vocabulary (a list, which reverses the vocabulary mapping).\n",
    "\n",
    "  Raises:\n",
    "    ValueError: if the provided vocabulary_path does not exist.\n",
    "  \"\"\"\n",
    "  if os.path.exists(vocabulary_path):\n",
    "    rev_vocab = []\n",
    "    with open(vocabulary_path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "      rev_vocab.extend(f.readlines())\n",
    "    \n",
    "    # TODO - CHANGE THE BELOW LINE\n",
    "    # rev_vocab = [tf.compat.as_bytes(line.strip()) for line in rev_vocab]\n",
    "    rev_vocab = [line.strip() for line in rev_vocab]\n",
    "    vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n",
    "    print(len(vocab.keys()), len(rev_vocab))\n",
    "    return vocab, rev_vocab\n",
    "  else:\n",
    "    raise ValueError(\"Vocabulary file %s not found.\", vocabulary_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This file contains code to read tokenized data from file,\n",
    "truncate, pad and process it into batches ready for training\"\"\"\n",
    "\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Batch(object):\n",
    "    \"\"\"A class to hold the information needed for a training batch\"\"\"\n",
    "\n",
    "    def __init__(self, context_ids, context_tokens, qn_ids, qn_mask, qn_tokens, ans_ids, ans_mask,\n",
    "                 ans_tokens, batch_size):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "          {context/qn}_ids: Numpy arrays.\n",
    "            Shape (batch_size, {context_len/question_len}). Contains padding.\n",
    "          {context/qn}_mask: Numpy arrays, same shape as _ids.\n",
    "            Contains 1s where there is real data, 0s where there is padding.\n",
    "          {context/qn/ans}_tokens: Lists length batch_size, containing lists (unpadded) of tokens (strings)\n",
    "          ans_span: numpy array, shape (batch_size, 2)\n",
    "        \"\"\"\n",
    "        self.context_ids = context_ids\n",
    "        # self.context_mask = context_mask\n",
    "        self.context_tokens = context_tokens\n",
    "        # self.context_embeddings = context_embeddings\n",
    "\n",
    "        self.qn_ids = qn_ids\n",
    "        self.qn_mask = qn_mask\n",
    "        self.qn_tokens = qn_tokens\n",
    "\n",
    "        self.ans_ids = ans_ids\n",
    "        self.ans_mask = ans_mask\n",
    "        self.ans_tokens = ans_tokens\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "\n",
    "def split_by_whitespace(sentence):\n",
    "    words = []\n",
    "    for space_separated_fragment in sentence.strip().split():\n",
    "        words.extend(re.split(\" \", space_separated_fragment))\n",
    "    return [w for w in words if w]\n",
    "\n",
    "\n",
    "def intstr_to_intlist(string):\n",
    "    \"\"\"Given a string e.g. '311 9 1334 635 6192 56 639', returns as a list of integers\"\"\"\n",
    "    return [int(s) for s in string.split()]\n",
    "\n",
    "\n",
    "def sentence_to_token_ids(sentence, word2id, is_instr=False):\n",
    "    \"\"\"Turns an already-tokenized sentence string into word indices\n",
    "    e.g. \"i do n't know\" -> [9, 32, 16, 96]\n",
    "    Note any token that isn't in the word2id mapping gets mapped to the id for UNK\n",
    "    \"\"\"\n",
    "    if is_instr:\n",
    "        tokens = instruction_tokenizer(sentence)  # list of strings\n",
    "    else:\n",
    "        tokens = split_by_whitespace(sentence)\n",
    "\n",
    "    # if simply split in tokens.\n",
    "    ids = [word2id.get(w, UNK_ID) for w in tokens]\n",
    "    ''' for debugging\n",
    "    if UNK_ID in ids:\n",
    "        print(tokens[ids.index(UNK_ID)], \" \".join(tokens))\n",
    "    '''\n",
    "    return tokens, ids\n",
    "\n",
    "def padded(token_batch, batch_pad=0):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "      token_batch: List (length batch size) of lists of ints.\n",
    "      batch_pad: Int. Length to pad to. If 0, pad to maximum length sequence in token_batch.\n",
    "    Returns:\n",
    "      List (length batch_size) of padded of lists of ints.\n",
    "        All are same length - batch_pad if batch_pad!=0, otherwise the maximum length in token_batch\n",
    "    \"\"\"\n",
    "    maxlen = max(map(lambda x: len(x), token_batch)) if batch_pad == 0 else batch_pad\n",
    "    return map(lambda token_list: token_list + [PAD_ID] * (maxlen - len(token_list)), token_batch)\n",
    "\n",
    "def reorganize(context_line, ans_line):\n",
    "    start = ans_line.strip().split()[0]\n",
    "    context_trip_list = context_line.strip().split(';')\n",
    "    trips_contain_start = []\n",
    "    trips_not_contain_start = []\n",
    "\n",
    "    for trip_str in context_trip_list:\n",
    "        if start in trip_str:\n",
    "            trips_contain_start.append(trip_str)\n",
    "        else:\n",
    "            trips_not_contain_start.append(trip_str)\n",
    "    if trips_not_contain_start[0][0] != ' ':\n",
    "        trips_not_contain_start[0] = ' ' + trips_not_contain_start[0]\n",
    "    organized_context_line = \";\".join(trips_contain_start + trips_not_contain_start).strip() + '\\n'\n",
    "\n",
    "    #assert len(organized_context_line) == len(context_line), \"len {} {}{} len {}\".\\\n",
    "    #      format(len(context_line), context_line, organized_context_line, len(organized_context_line))\n",
    "    return organized_context_line\n",
    "\n",
    "\n",
    "def refill_batches(batches, word2id, context2id, ans2id, context_file, qn_file, ans_file, batch_size, context_len,\n",
    "                   question_len, ans_len, discard_long, shuffle=True, output_goal=False):\n",
    "    \"\"\"\n",
    "    Adds more batches into the \"batches\" list.\n",
    "    Inputs:\n",
    "      batches: list to add batches to\n",
    "      word2id: dictionary mapping word (string) to word id (int)\n",
    "      context_file, qn_file, ans_file: paths to {train/dev}.{context/question/answer} data files\n",
    "      batch_size: int. how big to make the batches\n",
    "      context_len, question_len: max length of context and question respectively\n",
    "      discard_long: If True, discard any examples that are longer than context_len or question_len.\n",
    "        If False, truncate those exmaples instead.\n",
    "    \"\"\"\n",
    "    print(\"Refilling batches...\")\n",
    "    tic = time.time()\n",
    "    examples = []  # list of (qn_ids, context_ids, ans_span, ans_tokens) triples\n",
    "    context_line, qn_line, ans_line = context_file.readline(), qn_file.readline(), ans_file.readline()  # read the next line from each\n",
    "    # print(context_line,qn_line,ans_line)\n",
    "    while context_line and qn_line and ans_line:  # while you haven't reached the end\n",
    "\n",
    "        # Reorganize the map to make the nodes containing the start point comes at the front.\n",
    "        context_line = reorganize(context_line, ans_line)\n",
    "        # Convert tokens to word ids\n",
    "        context_tokens, context_ids = sentence_to_token_ids(context_line, context2id)\n",
    "        qn_tokens, qn_ids = sentence_to_token_ids(qn_line, word2id, is_instr=True)\n",
    "\n",
    "        ans_tokens, ans_ids = sentence_to_token_ids(ans_line, ans2id)\n",
    "\n",
    "        ############# reorganize ans tokens into [start] + [action list] (+ [end]) #####################\n",
    "        if output_goal:\n",
    "            ans_tokens = [ans_tokens[0]] + ans_tokens[1::2] + [ans_tokens[-1]]\n",
    "            ans_ids = [ans_ids[0]] + ans_ids[1::2] + [ans_ids[-1]]\n",
    "        else:\n",
    "            ans_tokens = [ans_tokens[0]] + ans_tokens[1::2]\n",
    "            ans_ids = [ans_ids[0]] + ans_ids[1::2]\n",
    "        ##############################################################################################s\n",
    "        \n",
    "\n",
    "        # read the next line from each file\n",
    "        context_line, qn_line, ans_line = context_file.readline(), qn_file.readline(), ans_file.readline()\n",
    "\n",
    "        # discard or truncate too-long questions\n",
    "        if len(qn_ids) > question_len:\n",
    "            if discard_long:\n",
    "                continue\n",
    "            else:  # truncate\n",
    "                qn_ids = qn_ids[:question_len]\n",
    "\n",
    "        # discard or truncate too-long contexts\n",
    "        if len(context_ids) > context_len:\n",
    "            if discard_long:\n",
    "                continue\n",
    "            else:  # truncate\n",
    "                context_ids = context_ids[:context_len]\n",
    "\n",
    "        # discard or truncate too-long answer\n",
    "        if len(ans_ids) > ans_len:\n",
    "            if discard_long:\n",
    "                continue\n",
    "            else:  # truncate\n",
    "                ans_ids = ans_ids[:ans_len]\n",
    "\n",
    "        # add to examples\n",
    "        examples.append((context_ids, context_tokens, qn_ids, qn_tokens, ans_ids, ans_tokens))\n",
    "\n",
    "        # stop refilling if you have 160 batches\n",
    "        if len(examples) == batch_size * 160:\n",
    "            break\n",
    "\n",
    "    # Once you've either got 160 batches or you've reached end of file:\n",
    "\n",
    "    # Sort by context length for speed\n",
    "    # Note: if you sort by context length, then you'll have batches which contain the same context many times\n",
    "    # (because each context appears several times, with different questions)\n",
    "    # shuffle==False means to not change the sequence of the input data, thus no sorting.\n",
    "    if shuffle:\n",
    "        examples = sorted(examples, key=lambda e: len(e[0]))\n",
    "\n",
    "    # Make into batches and append to the list batches\n",
    "    for batch_start in range(0, len(examples), batch_size):\n",
    "        # Note: each of these is a list length batch_size of lists of ints (except on last iter when it might be less than batch_size)\n",
    "        context_ids_batch, context_tokens_batch, qn_ids_batch, qn_tokens_batch, ans_span_batch, ans_tokens_batch = zip(*examples[batch_start:batch_start + batch_size])\n",
    "\n",
    "        batches.append(\n",
    "            (context_ids_batch, context_tokens_batch, qn_ids_batch, qn_tokens_batch, ans_span_batch, ans_tokens_batch))\n",
    "    if shuffle:\n",
    "        # shuffle the batches\n",
    "        random.shuffle(batches)\n",
    "\n",
    "    toc = time.time()\n",
    "    print(\"Refilling batches took %.2f seconds\" % (toc - tic))\n",
    "    return\n",
    "\n",
    "\n",
    "def get_batch_generator(word2id, context2id, ans2id, context_path, qn_path, ans_path, batch_size, graph_vocab_class,\n",
    "                        context_len, question_len, answer_len, discard_long, shuffle=True, output_goal=False):\n",
    "    \n",
    "    context_file, qn_file, ans_file = open(context_path, encoding=\"utf-8\"), open(qn_path, encoding=\"utf-8\"), open(ans_path, encoding=\"utf-8\")\n",
    "    batches = []\n",
    "\n",
    "\n",
    "    while True:\n",
    "        if len(batches) == 0:  # add more batches\n",
    "            refill_batches(batches, word2id, context2id, ans2id, context_file, qn_file, ans_file, batch_size,\n",
    "                           context_len, question_len, answer_len, discard_long, shuffle=shuffle, output_goal=output_goal)\n",
    "        if len(batches) == 0:\n",
    "            break\n",
    "\n",
    "        # Get next batch. These are all lists length batch_size\n",
    "        (context_ids, context_tokens, qn_ids, qn_tokens, ans_ids, ans_tokens) = batches.pop(0)\n",
    "\n",
    "        # Pad context_ids and qn_ids\n",
    "        qn_ids = padded(qn_ids, question_len)  # pad questions to length question_len\n",
    "        context_ids = padded(context_ids, context_len)  # pad contexts to length context_len\n",
    "        ans_ids = padded(ans_ids, answer_len) # pad ans to maximum length\n",
    "\n",
    "        # Make qn_ids into a np array and create qn_mask\n",
    "        qn_ids = np.array(list(qn_ids))  # shape (batch_size, question_len)\n",
    "        qn_mask = (qn_ids != PAD_ID).astype(np.int32)  # shape (batch_size, question_len)\n",
    "\n",
    "        # Make context_ids into a np array and create context_mask\n",
    "        context_ids = np.array(list(context_ids))  # shape (batch_size, context_len)\n",
    "        # context_mask = (context_ids != PAD_ID).astype(np.int32)  # shape (batch_size, context_len)\n",
    "\n",
    "        # Make ans_ids into a np array and create ans_mask\n",
    "        ans_ids = np.array(list(ans_ids))\n",
    "        ans_mask = (ans_ids != PAD_ID).astype(np.int32)\n",
    "        # print(list(ans_ids), list(context_ids), list(qn_ids))\n",
    "        # interpret graph as triplets and append the first token\n",
    "        # if not show_start_tokens:\n",
    "        # context_embeddings, context_mask = compute_graph_embedding(context_tokens, graph_vocab_class, context_mask.shape[1])\n",
    "        # else:\n",
    "        #     context_embeddings, context_mask = compute_graph_embedding(context_tokens, graph_vocab_class, context_mask.shape[1],\n",
    "        #                                                     np.array([ans_token[0] for ans_token in ans_tokens]))\n",
    "        \n",
    "        # Make into a Batch object\n",
    "        batch = Batch(context_ids, context_tokens, qn_ids, qn_mask, qn_tokens, ans_ids, ans_mask, ans_tokens, batch_size)\n",
    "        # print(len(batch))\n",
    "        yield batch\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Skipping generating vocabulary file for ./data/vocab200.context\n158 158\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import logging\n",
    "# import torch\n",
    "# from model import QAModel\n",
    "# from train import train\n",
    "import pickle\n",
    "from train import train\n",
    "from model import QAModel\n",
    "\n",
    "from preprocess import get_glove, create_vocabulary, create_vocab_class\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "NUM_EPOCH = 1\n",
    "LEARNING_RATE =0.001\n",
    "BATCH_SIZE = 1\n",
    "HIDDEN_SIZE = 128\n",
    "CONTEXT_LEN = 300\n",
    "QUESTION_LEN = 150\n",
    "ANSWER_LEN = 50\n",
    "EMBEDDING_SIZE = 100\n",
    "LOAD_PREV = False\n",
    "# SAMPLING CONSTANTS\n",
    "\n",
    "with open(\"./data/emb_matrix.pkl\", \"rb\") as f:\n",
    "    emb_matrix = pickle.load(f)\n",
    "\n",
    "with open(\"./data/word2id.pkl\", \"rb\") as f:\n",
    "    word2id=  pickle.load(f)\n",
    "\n",
    "with open(\"./data/id2word.pkl\", \"rb\") as f:\n",
    "    id2word = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "context_vocab_path = \"./data/vocab200.context\"\n",
    "train_context_path = \"./data/train.graph\"\n",
    "context_vocab, rev_context_vocab = create_vocabulary(context_vocab_path,train_context_path,200)\n",
    "NO_CLASS = len(context_vocab)\n",
    "\n",
    "# TODO \n",
    "# qa_model.train(train_context_path, train_qn_path, train_ans_path, dev_qn_path, dev_context_path, dev_ans_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/18110188/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.19999999999999996 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "/home/18110188/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "Refilling batches...\n",
      "Refilling batches took 0.74 seconds\n",
      "torch.Size([1, 300, 100]) \n",
      " tensor([[[-1.3227, -1.8362, -0.1194,  ...,  2.7579, -0.7590,  0.4183],\n",
      "         [ 0.5558,  2.9219, -0.5056,  ...,  0.8380,  0.8165, -1.6780],\n",
      "         [ 1.0702, -2.0564,  1.7779,  ..., -0.0565, -0.7917,  2.0733],\n",
      "         ...,\n",
      "         [ 0.5558,  2.9219, -0.5056,  ...,  0.8380,  0.8165, -1.6780],\n",
      "         [ 0.6609,  0.8935, -0.1438,  ...,  0.2432, -0.1613, -0.6995],\n",
      "         [ 0.9558, -0.8578, -1.0024,  ..., -0.9107, -1.1117, -0.0918]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "torch.Size([150, 1, 100]) \n",
      " tensor([[[-0.3423, -0.3097,  0.2258,  ...,  0.7432,  0.0752,  0.1242]],\n",
      "\n",
      "        [[-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706]],\n",
      "\n",
      "        [[-0.0248,  0.4777,  0.3244,  ..., -0.2263,  0.8283,  0.0585]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0964,  0.1843, -1.6782,  ...,  0.4969,  0.2281,  0.4635]],\n",
      "\n",
      "        [[-0.0964,  0.1843, -1.6782,  ...,  0.4969,  0.2281,  0.4635]],\n",
      "\n",
      "        [[-0.0964,  0.1843, -1.6782,  ...,  0.4969,  0.2281,  0.4635]]])\n",
      "torch.Size([1, 256, 150])\n",
      "attn logits:  torch.Size([1, 300, 150])\n",
      "attn dist:  torch.Size([1, 300, 150])\n",
      "torch.Size([128]) torch.Size([300, 1, 128]) torch.Size([1, 300, 128])\n",
      "torch.Size([1, 300, 128]) torch.Size([1, 300, 128])\n",
      "/home/18110188/.local/lib/python3.6/site-packages/torch/nn/functional.py:1339: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "ipykernel_launcher:96: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Sizes of tensors must match except in dimension 2. Got 50 and 1 in dimension 1 at /pytorch/aten/src/TH/generic/THTensor.cpp:689",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-38ed3a297d1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#     qa_model.load_state_dict(torch.load(\"./data\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqa_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./data/train.graph\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./data/train.instruction\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./data/train.answer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/PC/natural-control/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(qamodel, num_epochs, context_path, qn_path, ans_path, batch_size)\u001b[0m\n\u001b[1;32m     52\u001b[0m                                             \u001b[0mcontext_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                                             answer_len=50, discard_long=False):\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mbatch_loss\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtrain_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqamodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0mnum_iters\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PC/natural-control/train.py\u001b[0m in \u001b[0;36mtrain_iteration\u001b[0;34m(qamodel, batch, criterion, encoder_optimizer, decoder_optimizer)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mcontext_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mdecoder_outputs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mqamodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqn_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontext_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mans_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mqn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m    \u001b[0;31m#loss per batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdec_out\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-3d0368edcc65>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, qn_ids, context_ids, ans_ids, qn_mask)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mdecoder_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec_hidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mblended_reps_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m             \u001b[0mdecoder_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0;31m# topk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-3d0368edcc65>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, word_input, last_hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (1,B,V)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;31m# Combine embedded input word and attended context, run through RNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mrnn_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_embedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;31m#rnn_input = self.attn_combine(rnn_input) # use it in case your size of rnn_input is different\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 2. Got 50 and 1 in dimension 1 at /pytorch/aten/src/TH/generic/THTensor.cpp:689"
     ]
    }
   ],
   "source": [
    "qa_model = QAModel(id2word, word2id, emb_matrix, context_vocab, rev_context_vocab, context_vocab, HIDDEN_SIZE, EMBEDDING_SIZE, NO_CLASS,BATCH_SIZE)\n",
    "\n",
    "# file_handler = logging.FileHandler(os.path.join(\"./model\", \"log.txt\"))\n",
    "# logging.getLogger().addHandler(file_handler)\n",
    "\n",
    "# if LOAD_PREV:\n",
    "#     qa_model.load_state_dict(torch.load(\"./data\"))\n",
    "\n",
    "train(qa_model, NUM_EPOCH, \"./data/train.graph\", \"./data/train.instruction\", \"./data/train.answer\", BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Model\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,hidden_size,embedding_size,keep_prob):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.keep_prob = keep_prob\n",
    "        self.gru = nn.GRU(embedding_size,hidden_size,dropout=1-keep_prob,bidirectional=True)\n",
    "    \n",
    "    def forward(self, embedded_inputs):\n",
    "        print(embedded_inputs.shape,'\\n',embedded_inputs)\n",
    "        output,hidden = self.gru(embedded_inputs)\n",
    "        \n",
    "        return output\n",
    "        # output = [seq_len,batch_size,hidden_size*2]\n",
    "        # hidden = [2,batch_size,hidden_size]\n",
    "        \n",
    "        # can add Dropout layer\n",
    "\n",
    "# class Decoder(nn.Module):\n",
    "#     def __init__(self, batch_size, hidden_size, tgt_vocab_size, max_decoder_length, embeddings, \n",
    "#                 keep_prob, sampling_prob, schedule_embed=False, pred_method='greedy'):\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.projection_layer = nn.Linear(hidden_size,tgt_vocab_size)\n",
    "#         self.gru = nn.GRU(hidden_size,hidden_size)\n",
    "#         self.batch_size = batch_size\n",
    "#         self.embeddings = embeddings\n",
    "#         self.start_id = SOS_ID\n",
    "#         self.end_id = PAD_ID\n",
    "#         self.tgt_vocab_size = tgt_vocab_size\n",
    "#         self.max_decoder_length = max_decoder_length\n",
    "#         self.keep_prob = keep_prob\n",
    "#         self.schedule_embed = schedule_embed\n",
    "#         self.pred_method = pred_method\n",
    "#         self.beam_width = 9\n",
    "#         self.sampling_prob = sampling_prob\n",
    "\n",
    "#     def forward(self, blended_reps_final, encoder_hidden, decoder_emb_inputs, ans_masks, ans_ids, context_masks):\n",
    "#         start_ids = ans_ids[:,0]\n",
    "#         train_output = blended_reps_final\n",
    "#         context_lengths = torch.Tensor([context_masks.size(1)]*self.batch_size)\n",
    "#         decoder_lengths = torch.Tensor([context_masks.size(1)]*self.batch_size)\n",
    "\n",
    "#         # traininghelper vali lines\n",
    "\n",
    "#         pred_start_ids = ans_ids[:,0]\n",
    "\n",
    "#         # pred_helper vali lines\n",
    "\n",
    "\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        stdv = 1. / math.sqrt(self.v.size(0))\n",
    "        self.v.data.normal_(mean=0, std=stdv)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs, src_len=None):\n",
    "        '''\n",
    "        :param hidden: \n",
    "            previous hidden state of the decoder, in shape (layers*directions,B,H)\n",
    "        :param encoder_outputs:\n",
    "            encoder outputs from Encoder, in shape (T,B,H)\n",
    "        :param src_len:\n",
    "            used for masking. NoneType or tensor in shape (B) indicating sequence length\n",
    "        :return\n",
    "            attention energies in shape (B,T)\n",
    "        '''\n",
    "        encoder_outputs = encoder_outputs.transpose(0,1) # [T*B*H]\n",
    "        max_len = encoder_outputs.size(0)\n",
    "        this_batch_size = encoder_outputs.size(1)\n",
    "        H = hidden.repeat(max_len,1,1).transpose(0,1)\n",
    "        print(hidden.shape, encoder_outputs.shape,H.shape)\n",
    "        attn_energies = self.score(H,encoder_outputs) # compute attention score\n",
    "        \n",
    "        if src_len is not None:\n",
    "            mask = []\n",
    "            for b in range(src_len.size(0)):\n",
    "                mask.append([0] * src_len[b].item() + [1] * (encoder_outputs.size(1) - src_len[b].item()))\n",
    "            mask = cuda_(torch.ByteTensor(mask).unsqueeze(1)) # [B,1,T]\n",
    "            attn_energies = attn_energies.masked_fill(mask, -1e18)\n",
    "        \n",
    "        return F.softmax(attn_energies).unsqueeze(1) # normalize with softmax\n",
    "\n",
    "    def score(self, hidden, encoder_outputs):\n",
    "        encoder_outputs = encoder_outputs.transpose(0,1)\n",
    "        print(hidden.shape,encoder_outputs.shape)\n",
    "\n",
    "        energy = F.tanh(self.attn(torch.cat([hidden, encoder_outputs], 2))) # [B*T*2H]->[B*T*H]\n",
    "        energy = energy.transpose(2,1) # [B*H*T]\n",
    "        v = self.v.repeat(encoder_outputs.data.shape[0],1).unsqueeze(1) #[B*1*H]\n",
    "        energy = torch.bmm(v,energy) # [B*1*T]\n",
    "        return energy.squeeze(1) #[B*T]\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embed_size, output_size, n_layers=1, dropout_p=0.1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        # Define parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_size = embed_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        # Define layers\n",
    "        self.embedding_dec = nn.Embedding(output_size, embed_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.attn = Attn('concat', hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size + embed_size, hidden_size, n_layers, dropout=dropout_p)\n",
    "        #self.attn_combine = nn.Linear(hidden_size + embed_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, word_input, last_hidden, encoder_outputs):\n",
    "        '''\n",
    "        :param word_input: === decoder_input\n",
    "            word input for current time step, in shape (B)\n",
    "        :param last_hidden:=== decoder_hidden\n",
    "            last hidden stat of the decoder, in shape (layers*direction*B*H)\n",
    "        :param encoder_outputs:\n",
    "            encoder outputs in shape (T*B*H)\n",
    "        :return\n",
    "            decoder output\n",
    "        Note: we run this one step at a time i.e. you should use a outer loop \n",
    "            to process the whole sequence\n",
    "            '''\n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        word_input = torch.from_numpy(word_input).long()\n",
    "        word_embedded = self.embedding_dec(word_input).view(1, word_input.size(0), -1) # (1,B,V)\n",
    "        word_embedded = self.dropout(word_embedded)\n",
    "        # Calculate attention weights and apply to encoder outputs\n",
    "        attn_weights = self.attn(last_hidden[-1], encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs)  # (B,1,V)\n",
    "        # context = context.transpose(0, 1)  # (1,B,V)\n",
    "        # Combine embedded input word and attended context, run through RNN\n",
    "        rnn_input = torch.cat((word_embedded, context), 2)\n",
    "        #rnn_input = self.attn_combine(rnn_input) # use it in case your size of rnn_input is different\n",
    "        output, hidden = self.gru(rnn_input, last_hidden)\n",
    "        output = output.squeeze(0)  # (1,B,V)->(B,V)\n",
    "        # context = context.squeeze(0)\n",
    "        # update: \"context\" input before final layer can be problematic.\n",
    "        # output = F.log_softmax(self.out(torch.cat((output, context), 1)))\n",
    "        output = F.log_softmax(self.out(output))\n",
    "        # Return final output, hidden state\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "\n",
    "class BasicAttn(nn.Module):\n",
    "    def __init__(self, keep_prob, key_vec_size, value_vec_size):\n",
    "        super(BasicAttn,self).__init__()\n",
    "        self.keep_prob = keep_prob\n",
    "        self.key_vec_size = key_vec_size\n",
    "        self.value_vec_size = value_vec_size\n",
    "\n",
    "    def forward(self, values, values_mask, keys):\n",
    "        # values = torch.from_numpy(values).float() \n",
    "        values_mask = torch.from_numpy(values_mask).float() \n",
    "        # keys = torch.from_numpy(keys).float() \n",
    "        attn_logits_mask = torch.unsqueeze(values_mask, 1) # -> (batch_size, 1, num_values)\n",
    "        \n",
    "        w = torch.zeros(self.key_vec_size, self.value_vec_size)\n",
    "        w = nn.init.xavier_normal_(w)\n",
    "        values_t = torch.transpose(values, 0, 1) \n",
    "        values_t = torch.transpose(values_t, 1,2)# -> (batch_size, value_vec_size, num_values)\n",
    "        def fn(a, x):\n",
    "            return torch.matmul(x, w).detach().numpy()\n",
    "\n",
    "        list_ = [fn(8, keys[i, :,:]) for i in range(keys.shape[0])]\n",
    "        part_logits = torch.Tensor(list_) # (batch_size, num_keys, value_vec)\n",
    "        print(values_t.shape)\n",
    "        attn_logits = torch.bmm(part_logits, values_t) # -> (batch_size, num_keys, num_values)\n",
    "        _, attn_dist = masked_softmax(attn_logits, attn_logits_mask, dim = -1)\n",
    "        # attn_dist = attn_dist.transpose(1,2)\n",
    "        print('attn dist: ',attn_dist.shape)\n",
    "        # print(values.shape,values_t.shape)\n",
    "        output = torch.matmul(attn_dist, values.transpose(0,1))\n",
    "\n",
    "        return attn_dist, output\n",
    "\n",
    "\n",
    "class QAModel(nn.Module):\n",
    "    def __init__(self, id2word, word2id, emb_matrix, ans2id, id2ans, context2id,hidden_size, embedding_size, tgt_vocab_size,batch_size):\n",
    "        super(QAModel,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.tgt_vocab_size = tgt_vocab_size\n",
    "        self.id2word = id2word\n",
    "        self.word2id = word2id\n",
    "        self.ans_vocab_size = len(ans2id)\n",
    "        self.ans2id = ans2id\n",
    "        self.id2ans = id2ans\n",
    "        self.batch_size = batch_size\n",
    "        self.emb_matrix = emb_matrix\n",
    "        self.context2id = context2id\n",
    "        self.keep_prob = 0.8\n",
    "        self.embedding = nn.Embedding(len(context2id),embedding_size)\n",
    "        self.linear21 = nn.Linear(2*hidden_size,hidden_size)\n",
    "        self.linear41 = nn.Linear(4*hidden_size,hidden_size)\n",
    "        self.graph_vocab_class = create_vocab_class(context2id)\n",
    "        self.context_dimension_compressed = len(self.graph_vocab_class.all_tokens) + len(self.graph_vocab_class.nodes)\n",
    "\n",
    "        self.encoder = Encoder(self.hidden_size,self.embedding_size, self.keep_prob)\n",
    "        self.decoder = DecoderRNN(hidden_size,embedding_size,tgt_vocab_size)\n",
    "\n",
    "    def forward(self,qn_ids,context_ids,ans_ids,qn_mask):\n",
    "        self.context_embs = self.embedding(context_ids)\n",
    "        context_hiddens = self.encoder(self.context_embs)  # (batch_size, context_len, hidden_size*2)\n",
    "\n",
    "        self.qn_embs = self.get_embeddings(self.id2word,self.emb_matrix,qn_ids,self.embedding_size,self.batch_size)\n",
    "        question_hiddens = self.encoder(self.qn_embs)  # (batch_size, question_len, hidden_size*2)\n",
    "        question_last_hidden = question_hiddens[:, -1, :]\n",
    "        question_last_hidden = self.linear21(question_last_hidden)\n",
    "\n",
    "        # Working fine till here\n",
    "\n",
    "        attn_layer = BasicAttn(self.keep_prob, self.hidden_size * 2, self.hidden_size * 2)\n",
    "        _, attn_output = attn_layer(question_hiddens, qn_mask, context_hiddens)\n",
    "        # Concat attn_output to context_hiddens to get blended_reps\n",
    "        blended_reps = torch.cat((context_hiddens, attn_output), axis=2)  # (batch_size, context_len, hidden_size*4)\n",
    "        blended_reps_final = self.linear41(blended_reps)\n",
    "        self.dec_hidden = question_last_hidden\n",
    "        # Idhar for loop lagaane ka hai\n",
    "        decoder_outputs = []\n",
    "        for idx in range(len(ans_ids)):\n",
    "            self.dec_output,self.dec_hidden = self.decoder(ans_ids[idx],self.dec_hidden,blended_reps_final)\n",
    "            decoder_outputs.append(self.dec_output)\n",
    "            # topk\n",
    "            # loss add\n",
    "        return self.decoder_outputs #, loss\n",
    "        \n",
    "        # ----------------------------------- #\n",
    "        \n",
    "    def get_embeddings(self,token2id,embed_matrix,input_ids,embed_size,batch_size):\n",
    "            array = np.zeros((len(input_ids[0]),batch_size,embed_size)) \n",
    "            # input_ids = [bsz,src_len]\n",
    "            # print(input_ids)\n",
    "            for idx,tokenised_words in enumerate(input_ids):\n",
    "                # words = [token2id[char_id] for char_id in tokenised_id]\n",
    "                for word_idx,word in enumerate(tokenised_words):\n",
    "                    array[word_idx,idx,:] = embed_matrix[word,:]\n",
    "            vector = torch.from_numpy(array).float()\n",
    "            # print(vector.size,vector)\n",
    "            return vector\n",
    "\n",
    "\n",
    "\n",
    "def masked_softmax(logits, masks, dim):\n",
    "    print('attn logits: ',logits.shape)\n",
    "    inf_mask = (1 - masks.type(torch.FloatTensor)) * (-1e30)\n",
    "    masked_logits = torch.add(logits, inf_mask)\n",
    "    sm = nn.Softmax(dim)\n",
    "    softmax_out = sm(masked_logits)\n",
    "    return masked_logits, softmax_out\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.3 64-bit ('gpu_caffe_env': conda)",
   "metadata": {
    "interpreter": {
     "hash": "a987dd0d0235cc48623cc4396b90af5f1cb3c1854e47a279645ffefac77ca3a4"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}